{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSk47i-iWQlv"
      },
      "source": [
        "# CSCI E-25     \n",
        "\n",
        "## Introduction to Deep Neural Networks\n",
        "\n",
        "## Steve Elston\n",
        "\n",
        "## 1.0 Overview\n",
        "\n",
        "This lesson introduces you to the basics of neural network architecture in the form of deep forward networks. This architecture is the quintessential deep neural net architecture. In this lesson you will master the following:\n",
        "\n",
        "- Why is deep learning important and how it relates to representation, learning and inference.\n",
        "- How a basic Preceptron works.\n",
        "- How to apply different types of loss functions.\n",
        "- Understand why nonlinear activation is important and why rectified linear units are a good choice.\n",
        "- How back propagation works, and how you apply the chain rule of calculus to determine gradient.\n",
        "- Understand the architectural trade-off between depth and width in deep networks.\n",
        "- Know how and why you must apply regularization to deep neural networks.  \n",
        "- The importance of learning rate in training deep neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOOCkUWyWQlx"
      },
      "source": [
        "### 1.1 Why is deep learning important?\n",
        "\n",
        "Deep learning methods are a form of **artificial intelligence (AI)** or **machine intelligence**. More specifically, deep learning algorithms are a type of **machine learning**.\n",
        "\n",
        "What properties does machine intelligence require? There have been many answers to this question over the history of computing. In this case, we will take a practical view, sometimes known as **weak AI**. There are three key properties an intelligent machine must have. Deep learning algorithms are one of a few classes of algorithms that can do the following, essential to machine intelligence:\n",
        "\n",
        "1. **Representation:** An intelligent machine must be able to represent a model of the world it interacts with in a general manner. Representation is key to intelligence. Without a good representation the best learning and inference algorithms will struggle. Whereas, good representation can greatly facilitate learning and inference. In conventional machine learning the representation is model and a set of features. The representation is limited to what the features can provide directly. Deep learning algorithms, on the other hand, learn learn complex representations from raw features. This behavior allows deep learning algorithms to approximate complex relationships. Further, the representations learned often generalize well, up to a point.\n",
        "2. **Learning:** As you likely guessed from the very name, deep learning algorithms learn from data. Whereas, conventional machine learning is focused on inference,deep learning algorithms learn both inference and representations. As a result, deep leaning algorithms are more complex and therefore harder to train than conventional machine learning algorithms.  \n",
        "3. **Inference:** Any machine intelligence algorithm must be able to perform inference. The inference is the result produced given new input data. To be useful, the inferences produced by a machine intelligence algorithm must **generalize** beyond the cases used for learning or training. Good generalization requires both good representations and learning which can deal with the complexity of diverse situations. Some deep learning algorithms can approach human levels of performance in inference tasks such as recognizing objects in images or understanding natural speech.\n",
        "\n",
        "The figure below shows a highly abstracted view of machine intelligence, showing the relationship between representation, learning and inference. In simple terms, the representation is learned and then used to make inferences. Errors in the inferences can be used to improve the learning of the representation.   \n",
        "\n",
        "<img src=\"https://github.com/whendo/CSCI-E25/blob/assignment5/Labs/img/MachineIntelligence.JPG?raw=1\" alt=\"Drawing\" style=\"width:600px; height:300px\"/>\n",
        "\n",
        "<center>Schematic for creating machine intelligence</center>\n",
        "\n",
        "**That's it!** The entire rest of this course will focus on just these three points: representation, learning and inference!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4U-UfmEqWQlx"
      },
      "source": [
        "### 1.2 Installing Keras\n",
        "\n",
        "This notebook will provides a first look at using the Keras package to define, train and evaluate deep learning models with Keras. The Keras package is a wrapper on TensorFlow, intended to abstract and simplify the definition, training and execution of TensorFlow deep learning models. You can find extensive well-written documentation for Keras [here](https://keras.io/). The book [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python-second-edition) by François Chollet, the creator of Keras, provides in-depth examples and discussion on a wide range of deep learning applications.      \n",
        "\n",
        "By the end of this lesson you will be able to work with basic feedforward architecture multi-layer neural nets. Feedforward networks are one of a class of basic models called **sequential models** which are easy to define with Keras. Some basic regularization is introduced. Additional regularization methods are covered in a subsequent lesson.\n",
        "\n",
        "Keras is part of the base package, as of the release of TensorFlow 2. Before proceeding make sure you have TensorFlow 2 installed in your environment. [Follow these instructions.](https://www.tensorflow.org/install).    \n",
        "\n",
        "****\n",
        "**Note:** As an alternative to working with a local installation, you may choose to use the [Google Colabratory](https://colab.research.google.com/?utm_source=scs-index). The Colabrotory virtual environment includes Anaconda, TensorFlow and Keras. However, the use of shared resources can result in slow execution.    \n",
        "****\n",
        "\n",
        "****\n",
        "**Note:** This notebook was constructed and tested using Anaconda 3 with Python 3. It is assumed that the standard Anaconda stack has been installed.\n",
        "****"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install np_utils"
      ],
      "metadata": {
        "id": "gpmxnG-TW1q5",
        "outputId": "67af21a2-d74f-4ade-98e0-1b87e756f949",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting np_utils\n",
            "  Downloading np_utils-0.6.0.tar.gz (61 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/62.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.0 in /usr/local/lib/python3.10/dist-packages (from np_utils) (1.25.2)\n",
            "Building wheels for collected packages: np_utils\n",
            "  Building wheel for np_utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for np_utils: filename=np_utils-0.6.0-py3-none-any.whl size=56439 sha256=52aa33398841b66f920e79a7a2d51e01c9e39cc4621ae7808a15e9d1a3b9e1ff\n",
            "  Stored in directory: /root/.cache/pip/wheels/b6/c7/50/2307607f44366dd021209f660045f8d51cb976514d30be7cc7\n",
            "Successfully built np_utils\n",
            "Installing collected packages: np_utils\n",
            "Successfully installed np_utils-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Se321hTJWQly"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist\n",
        "import tensorflow as tf\n",
        "import np_utils as ku\n",
        "## Depending on your environment and verison of Keras and Tensorflow you may need to\n",
        "## substitue the following import for the foregoing'\n",
        "## from keras import np_utils as ku\n",
        "import keras.models as models\n",
        "import keras.layers as layers\n",
        "from keras.optimizers import RMSprop\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "from keras import regularizers\n",
        "import numpy as np\n",
        "from math import exp\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDaGqm-2WQlz"
      },
      "source": [
        "## 2.0 Forward propagation: The representation problem\n",
        "\n",
        "To create useful neutral network we need a **representation** that has two important properties.   \n",
        "\n",
        "First, there needs to be a way to represent complex functions of the input. Without this property, nothing is gained, since there are numerous machine learning algorithms that work with simple representations. We will spend the rest of this section exploring this problem.   \n",
        "\n",
        "Second, the representation needs to be learnable. Quite obviously, no machine intelligence representation is useful if there is not a practical algorithm to learn it. We will take up this problem in another section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouphkK-GWQlz"
      },
      "source": [
        "### 2.1 Linear networks\n",
        "\n",
        "Let's start with the simplest possible network. It has inputs, and an output. The output is a **afine transformation** of the input values. We say this network performs an afine transformation since there is a bias term $b$.\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/whendo/CSCI-E25/blob/assignment5/Labs/img/LinearNetwork.JPG?raw=1\" alt=\"Drawing\" style=\"width:400px; height:250px\"/>\n",
        "\n",
        "<center>**Figure 2.1**\n",
        "**A simple afine network**</center>\n",
        "\n",
        "This output $y$ of this network is just:\n",
        "\n",
        "$$y = f(x) = \\sum_i w_i \\cdot x_i + b$$\n",
        "\n",
        "This network performs linear regression. Being able to perform only afine transformations, it can't do anything else.\n",
        "\n",
        "This representation is certainly learnable. However, it does not gain us anything over familiar linear regression methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y9CtluVWQlz"
      },
      "source": [
        "### 2.2 The preceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6eRbNCFWQlz"
      },
      "source": [
        "To get started, let's have a look at a simple **preceptron** model. The perceptron was proposed by Rosenblatt (1962). He built on the earlier attempts at a neural network models by McCulloch and Pitts (1943) and Heeb (1949). The perceptron adds **nonlinear activation** to the afine network.\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/whendo/CSCI-E25/blob/assignment5/Labs/img/Preceptron.JPG?raw=1\" alt=\"Drawing\" style=\"width:350px; height:250px\"/>\n",
        "<center>Figure 2.2 Schematic of perceptron with nonlinear activation</center>\n",
        "\n",
        "The output $y$ of the perceptron is given by the following:\n",
        "\n",
        "$$y = f(x) = \\sigma \\Big( \\sum_i w_i \\cdot x_i + b \\Big)$$\n",
        "\n",
        "The output of the network is now nonlinear, give the **activation function** $\\sigma(x)$.\n",
        "\n",
        "But, the preceptron is nothing more than a logistic regression classifier. The fact that the preceptron could only solve linearly separable problems was pointed out by Minsky and Papert (1969). The failure of the preceptron to learn an **exclusive or (XOR)** function is well known. See for example, Section 6.1 in GBC.\n",
        "\n",
        "Again, this representation is certainly learnable. However, as before, it does not gain us anything over well known logistic regression models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0g02WKfWQlz"
      },
      "source": [
        "### 2.3 Forward networks - We're gonna need a better representation!\n",
        "\n",
        "The problem with the  perceptron is one of representations. There is no way that this simple network can represent anything but a linearly separable function. To represent more complex functions, we need a more complex network. In more technical terms we need a network with greater **model capacity**.\n",
        "\n",
        "What we need is a network with layers of **hidden nodes**. The figure below shows a simple example of a neural network with one **hidden layer** with two nodes. Since every node (including inputs) is connected to every other node we call this architecture a **fully connected neural network**.\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/whendo/CSCI-E25/blob/assignment5/Labs/img/Hidden.JPG?raw=1\" alt=\"Drawing\" style=\"width:600px; height:350px\"/>\n",
        "<center>**Figure 2.3  \n",
        "Fully connected neural network with single hidden layer**</center>\n",
        "\n",
        "Let's walk through some aspects of these diagrams.\n",
        "\n",
        "1. The neural network is divided into three layers. The input layer, the hidden layer and the output layer.\n",
        "2. The values in the input layer are multiplied by a weight matrix, $W^1$.\n",
        "3. The nodes in the hidden layer sum their inputs and add a bias term, $b^1$.\n",
        "4. The outputs of the hidden layer nodes are multiplied by a weight vector, $W^2$.\n",
        "5. The output layer sums the inputs and adds another bias term, $b^2$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EsEXn9YWQl0"
      },
      "source": [
        "### 2.4 Neural network architectures - Finding representations\n",
        "\n",
        "The representations achievable by neural network with just a single hidden layer  are quite powerful. In fact, Cybenko (1989) showed that such a network with an infinite number of hidden units using sigmoidal activation can approximate any arbitrary function. Hornik (1991) generalized this to apply to any activation function. We call this theorem the **universal approximation theorem**.  \n",
        "\n",
        "A universal approximation theorem may see like a really exciting development; especially if you are a machine intelligence nerd. However, one must be circumspect when viewing such a result. A representation with an infinite number of nodes cannot be learned in any practical sense. Still it is comforting to know that, at least in principle, a representation can be learned for arbitrarily complex problems.\n",
        "\n",
        "While infinitely wide networks with a single layer are unrealistic, we are not limited to one dimension. In fact, depth is typically more effective at creating complex representations rather than width in neural networks. Depth is measured by the count of hidden layers stacked one on top of the other in the network. Hence, the term deep neural networks.\n",
        "\n",
        "The Figure 2.4 below shows the results of an empirical study by Goodfellow, Shlens and Szegedy (2014) of accuracy of the network vs depth. Notice that accuracy increases rapidly with depth until about 8 layers, after which the effect is reduced.\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/whendo/CSCI-E25/blob/assignment5/Labs/img/Accuracy-Layers.JPG?raw=1\" alt=\"Drawing\" style=\"width:600px; height:350px\"/>\n",
        "<center>Figure 2.4 Empirical results of accuracy vs. number of layers  \n",
        "Diagram from Goodfellow et. al. 2014</center>\n",
        "\n",
        "Another view of the empirical study by Goodfellow et. al. is shown in Figure 2.5 below. In this case accuracy verses number of model parameters is compared for three different network architectures. The deeper network (11 layers) makes more  efficient use of the parameters in terms of improved accuracy. The number of parameters in a layer is approximately the total number of parameters divided by the number of layers. Notice that for the particular case tested convolutional neural networks are more efficient than fully-connected networks. We will discuss convolutional neural networks in a subsequent lesson.\n",
        "\n",
        "Of particular interest is the fact that the fully-connected network and the shallow convolutional neural network appear to be over-fitting as the test accuracy actually decreases as the number of parameters increases. We will discuss the significant problems of over-fitting in neural networks in a subsequent lesson.\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/whendo/CSCI-E25/blob/assignment5/Labs/img/Accuracy-Parameters.JPG?raw=1\" alt=\"Drawing\" style=\"width:600px; height:350px\"/>\n",
        "<center>**Figure 2.5 Empirical results of accuracy for different network architectures**  \n",
        "Diagram from Goodfellow et. al. 2014</center>\n",
        "\n",
        "**Summary:** Deep networks tend to produce better models, with less tendency to over-fit, for a given level of complexity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faTw8byuWQl0"
      },
      "source": [
        "### 2.5 Computational graphs\n",
        "\n",
        "There is another way to look at neural nets, computational graphs. A computational graph breaks down the steps of a complex algorithm into steps.\n",
        "\n",
        "Computational graphs provide a way to organize complex computations in an efficient manner. Widely used computational frameworks such as Tensor Flow, CNTK, and Torch all use computational graphs. Organizing computations in a graph allows these platform to minimize memory transfers. In simple terms, the platform can look ahead in the graph and organize data and computational results so as to minimize memory transfers. As a result, such platforms can be significantly faster than, say, Python Numpy. Systems like Numpy require memory transfer before each operation, which typically take more time than the actual computation.\n",
        "\n",
        "The diagram below decomposes the single hidden layer neural network discussed in the previous section into a computational graph.\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/whendo/CSCI-E25/blob/assignment5/Labs/img/CompGraph1.JPG?raw=1\" alt=\"Drawing\" style=\"width:600px; height:300px\"/>\n",
        "<center>**Figure 2.6  \n",
        "Computational graph for fully connected neural network of Figure 2.3** </center>\n",
        "\n",
        "Let's walk though this graph, step by step.\n",
        "\n",
        "1. The $nX2$ weight tensor, $W^1$, is multiplied by the 1-dimensional input tensor $x \\in\\ R^n$, giving  the result $U^1 \\in\\ R^2$.\n",
        "2. The 1-dimensional bias tensor $b^1 \\in\\ R^2$ is added to $U^1$, giving $U^2 \\in\\ R^2$.\n",
        "2. The activation function $\\sigma_h(x)$ is applied to $U^2$, producing $U^3 \\in\\ R^2$\n",
        "3. The dot product between the weight tensor, $W^2 \\in\\ R^2$ and $U^3$ is computed giving $U^4 \\in\\ R^1$.\n",
        "4. The bias, $b^2 \\in\\ R^1$ is added to $U^4$ giving $U^5 \\in\\ R^1$.\n",
        "5. The output activation function $\\sigma_o(x)$ is applied to $U^4$ giving the output $Y \\in\\ R^1$.\n",
        "\n",
        "As you can see, the computational graph provides a complete specification for the single hidden layer neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QINIgrhtWQl0"
      },
      "source": [
        "### 2.6 Activation functions\n",
        "\n",
        "Without a nonlinear activation function, a neural net is just an afine transformation. Afine transformations limit representation to only linearly separable functions. To create more general representations **nonlinear activation functions** are required.\n",
        "\n",
        "In present practice, four types of activation functions are generally used for fully connected networks.\n",
        "\n",
        "1. **Linear** activation is used for the output layer of regression neural networks.\n",
        "2. The **rectilinear** activation function is used for most hidden units. The rectilinear activation function is often referred to as **ReLU**.\n",
        "3. A **leaky rectilinear** activation acts like a ReLU function for positive inputs, but has a small negative bias or leakage for negative input values. The leaky ReLU activation function can improve training for some deep neural networks.\n",
        "3. The **logistic** or **sigmoid** activation function is used for binary classifiers.\n",
        "4. The **softmax** activation function is used for multi-class classifiers.\n",
        "\n",
        "Rectilinear functions are typically used as the activation function for hidden units in neural networks. The rectilinear function is defined at:\n",
        "\n",
        "$$f(x) = max(0, x)$$\n",
        "\n",
        "The rectilinear function is linear for positive responses and zero for responses less than 0.0. Notice that the derivatives of the rectilinear function are not continuous. While this might seem to be a problem, in practice, even gradient-based optimization functions work well with this activation function.\n",
        "\n",
        "The rectilinear function is plotted in the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zzYEPh7AWQl0",
        "outputId": "3136df2f-c184-4bad-e4d2-f2f4e1c0e5e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "def reclu(x): return(max(0,x))\n",
        "\n",
        "def plot_figs(x,y,title, figsize = (8, 6)):\n",
        "    plt.figure(figsize=figsize).gca() # define axis\n",
        "    sns.set_style(\"darkgrid\")\n",
        "    plt.plot(x, y)\n",
        "    plt.ylim((-0.1,1.1))\n",
        "    plt.title(title)\n",
        "    plt.xlabel('X')\n",
        "    plt.ylabel('Y')\n",
        "\n",
        "x = np.linspace(-1.0, 1.0, 200)\n",
        "y = [reclu(y) for y in x]\n",
        "plot_figs(x,y,'The Rectilinear Function')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm26ulGYWQl1"
      },
      "source": [
        "Another widely used activation function is the **logistic** or **sigmoid**. The sigmoid is used as the activation for the output layer of a binary classifier. The general sigmoid function can be written as:\n",
        "\n",
        "$$\\sigma(x) = \\frac{L}{1 + e^{-k(x_0-x)}}\\\\\n",
        "where\\\\\n",
        "L = max\\ value\\\\\n",
        "k = slope\\\\\n",
        "x_0 = sigmoid\\ midpoint$$\n",
        "\n",
        "With $L=1$, $k=1$, and $x_0 = 0$, the logistic function becomes:\n",
        "\n",
        "$$\\sigma(x) = \\frac{1}{1 + e^{-x}} = \\frac{e^x}{1+e^x}$$\n",
        "\n",
        "The sigmoid function can asymptotically approach $0$ or $1$, but will never reach these extreme values. However, because of the rapid decrease in the derivative away from $0$ the sigmoid can **saturate** when using gradient-based training. For this reason, the sigmoid is typically not used for hidden layers in neural networks.   \n",
        "\n",
        "When used in a the binary classifier a threshold is set to determine if the result is $0$ or $1$. The threshold can be adjusted to bias the result as desired.\n",
        "\n",
        "The code in the cell below plots the sigmoid function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "acJ7132sWQl1",
        "outputId": "23109f9d-6e05-408f-d7dc-38df19f8365b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "def sigmoid(x): return exp(x)/(1 + exp(x))\n",
        "\n",
        "x = np.linspace(-8.0, 8.0, 200)\n",
        "y = [sigmoid(y) for y in x]\n",
        "plot_figs(x,y,'The Logistic Function') #, figsize = (5,3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrYIU0rFWQl1"
      },
      "source": [
        "The **softmax** function or **normalized exponential function** is used for the output activation function of a multi-class classifiers. The softmax function is the multinomial generalization of the sigmoid or logistic function. The probability of each class $j$ is written as:\n",
        "\n",
        "$$\\sigma(z_j) = \\frac{e^{z_j}}{\\sum_{k=1}^K e^{z_k}}$$\n",
        "\n",
        "The normalization $\\sum_{k=1}^K e^{z_k}$ ensures the sum of probabilities for all classes add to $1.0$. The class selected by the classifier is the class with the largest value of $\\sigma(z_j)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA3yIr8NWQl1"
      },
      "source": [
        "### 2.7 Computational example\n",
        "\n",
        "Now that we have gone though some basic theory for feed-forward networks, it's time try a simple example. You will construct a fully connected network to compute this simple function:\n",
        "\n",
        "$$y = x_1 - x_2$$\n",
        "\n",
        "****\n",
        "**Comment.** You have likely have noticed that this function is linear and can be computed easily without a neural network. Of course, that is not the point. We use a simple function to make the results easy to understand.\n",
        "****\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Zkqu3MtWQl1"
      },
      "source": [
        "> **Exercise 5-1:** You will create and test a simple neural network implemented using matrix multiplication with Numpy. The architecture of the neural network is similar to the one shown in Figure 2.3, with 2 input units, 2 hidden units and 1 output unit. There are a total of 6 weights in two tensors. The neural network for this example does not require any bias terms.\n",
        "> 1. As a first step, create test data for 3 possibilities; $x_1 > x_2$, $x_1 = x_2$, and $x_1 <x_2$, and with positive and negative values, or $x = [(2,1), (1,1), (1,2), (0,0), (2,-1), (-1,-1), (-2,1), (-1,-2)]$ as the input tuples.\n",
        "> 2. Directly compute and print the evaluation of the function, $y = x_1 - x_2$, for each tuple.  \n",
        "\n",
        "> **Note:** The network you are asked to construct is simple and all weights must in the set $\\{-1, 1 \\}$. You can take advantage of the symmetry of the function you must approximate to determine these weights by inspection. If you wish, you will find it easy to compute the partial derivatives of the function to be approximated. However, this is not necessary if you carefully inspect the network and consider the responses required.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jM5jw_dBWQl1",
        "outputId": "da5fed5e-55ce-4e86-d708-2d3f05ca57f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "0\n",
            "-1\n",
            "0\n",
            "3\n",
            "0\n",
            "-3\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "## Your code goes here\n",
        "x = [(2,1), (1,1), (1,2), (0,0), (2,-1), (-1,-1), (-2,1), (-1,-2)]\n",
        "for x_in in x:\n",
        "    print(x_in[0] - x_in[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRyxZBunWQl2"
      },
      "source": [
        "> 3. Now that you have the test data you can move to the next step. Determine the values of the $2 \\times 2$ weight tensor between the input layer and the hidden layer (input tensor) and the $2 \\times 1$ weight tensor between the hidden and output layer (output tensor). In the code cells below create as Numpy arrays and print these tensors. *Hint:* Keep in mind that the input tensor must be symmetric, with correct signs on the $\\{-1,1\\}$ weights. Likewise, the output tensor $\\{-1,1\\}$ weights must have the opposite signs.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_xEQQh3MWQl2",
        "outputId": "f0476be0-73d4-438c-a47a-bf27ff3476fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1. -1.]\n",
            " [-1.  1.]]\n"
          ]
        }
      ],
      "source": [
        "## Your code goes here\n",
        "W_1 = np.array([[1.0, -1.0], [-1.0, 1.0]])\n",
        "print(W_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4PhuB-4iWQl2",
        "outputId": "6a2a884a-52e1-4e13-cba3-19395eeaecdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1 -1]\n"
          ]
        }
      ],
      "source": [
        "## Your code goes here\n",
        "W_2 = np.array([1, -1])\n",
        "print(W_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ytE8KO0WQl2"
      },
      "source": [
        "> 4. Now, it is time to compute the results and check them. To create the computational process follow the graph in Figure 2.6, but ignoring the bias terms, $b^1$ and $b^2$.\n",
        "> Create a function, `hidden`, to compute the output of the hidden layer using the formulation with **rectalinear activation**, $\\delta()$:   \n",
        "> $$h = \\delta(W^1 \\cdot x)$$\n",
        "> Create a second function ,`output`, to computes the vector product of the weight vector with the output vector of the hidden layer using a **linear activation**:    \n",
        "> $$o = W^2 \\cdot h$$\n",
        "> 5. Execute the two functions while iterating over the input tuples and verify the output is correct. If not, reconsider the values of your weight tensors.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tYA7oOjtWQl2",
        "outputId": "eec8e00b-5cc3-42e2-9231-1297290a5dc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "0\n",
            "-1.0\n",
            "0\n",
            "3.0\n",
            "0\n",
            "-3.0\n",
            "1.0\n"
          ]
        }
      ],
      "source": [
        "## Your code goes here\n",
        "def hidden(x, W):\n",
        "    \"\"\"Computes the output of the hidden layer\"\"\"\n",
        "    h = np.dot(W, x) # product of weights and input vector\n",
        "    return np.array([reclu(x) for x in h]) # apply activation function and return\n",
        "\n",
        "def output(h, W):\n",
        "    \"\"\"Computes the result for the hidden layer\"\"\"\n",
        "    return np.dot(W, h) # dot product of weight vector and input vector\n",
        "\n",
        "## Run the test cases and check the results\n",
        "for y in x:\n",
        "        h = hidden(y, W_1)\n",
        "        print(output(h, W_2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CT7i1GLGWQl2"
      },
      "source": [
        "> If your results agree with the function created above, congratulations! Your first fully connected neural network passed all the tests!  \n",
        "> **End of exercise**.\n",
        "\n",
        "Notice that even a network to compute a simple function requires 6 weights. You can see that for more complex functions any practical algorithm must learn a large number of weights. The limitations of Numpy would quickly become evident for large scale problems involving hundreds of millions of weights.\n",
        "\n",
        "****\n",
        "**Note:** If you are having difficulty following the Numpy code in the above example, you might want to look at [Scott Shell's Numpy Tutorial](https://engineering.ucsb.edu/~shell/che210d/numpy.pdf)\n",
        "****"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUeRMKy3WQl2"
      },
      "source": [
        "> **Exercise 5-2:** You will now construct and test an neural network implementing an [exclusive or function, the XOR](https://en.wikipedia.org/wiki/Exclusive_or). The XOR function outputs a 1 if either input is 1 and the other 0, and a 0 otherwise. The truth table for the XOR function is:   \n",
        "\n",
        "$$\\begin{bmatrix}\n",
        "in_1 & in_2 &  out\\\\         \n",
        "0 & 0 & 0\\\\   \n",
        "1 & 0 & 1\\\\    \n",
        "0 & 1 & 1\\\\    \n",
        "1 & 1 & 0\n",
        "\\end{bmatrix}$$     \n",
        "\n",
        "\n",
        "> You can use the `hidden` and `output` functions you created for the previous exercise with new weight tensors. Make sure you try all 4 possible test cases.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "8hFR67CFWQl2",
        "outputId": "a49672c0-feab-4647-f30b-fdecdc97f39f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1.0\n",
            "1.0\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "x = [(0,0), (1,0), (0,1), (1,1)]\n",
        "\n",
        "W_1 = np.array([[1.0, -1.0], [-1.0, 1.0]])\n",
        "\n",
        "W_2 = np.array([1, 1])\n",
        "\n",
        "## Run the test cases and check the results\n",
        "for y in x:\n",
        "        h = hidden(y, W_1)\n",
        "        print(output(h, W_2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJ7MsOyXWQl3"
      },
      "source": [
        "> If your output agrees with the truth table, congratulations! You have solved the XOR problem using nonlinear activation.       \n",
        "> **End of exercise.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2D7CCPJWQl3"
      },
      "source": [
        "## 3.0 Learning in neural networks: Backpropagation\n",
        "\n",
        "Now that we have a promising representation, we need to determine if it is trainable. The answer is not only yes we can, but that we can do so in a computationally efficient manner, using a cleaver algorithm known as **backpropagation**.\n",
        "\n",
        "The backpropagation algorithm was developed independently multiple times. The earliest work on this algorithm was by Kelly (1960) in the context of control theory and Bryson (1961) in the context of dynamic programming. Rumelhart, Hinton and Williams (1984) demonstrated empirically that backpropagation can be used to train neural networks. Their paper marks the beginning of the modern history of neural networks, and set off the first wave of enthusiasm.\n",
        "\n",
        "The backpropagation algorithm requires several components. First, we need a **loss function** to measure how well our representation matches the function we are trying to learn. Second, we need a way to propagate changes in the representation through the complex network For this we will use the **chain rule of calculus** to compute **gradients** of the representation. In the general case, this process requires using automatic differentiation methods.\n",
        "\n",
        "The point of backpropagration is to learn the optimal weight for the neural network. The algorithm proceeds iteratively through a series of small steps. Once we have the gradient of the loss function we can update the tensor of weights.\n",
        "\n",
        "$$W_{t+1} = W_t + \\alpha \\nabla_{W} J(W_t) $$  \n",
        "where  \n",
        "$W_t = $ the tensor of weights or model parameters at step $t$.   \n",
        "$\\alpha\\ = $ step size or learning rate.  \n",
        "$J(W) = $ loss function given the weights.  \n",
        "$\\nabla_{W} J(W) = $ gradient of $J$ with respect to the weights $W$.  \n",
        "\n",
        "It should be evident that the back propagation algorithm is a form of gradient decent. The weights are updated in small steps following the gradient of $J(W)$ down hill.\n",
        "\n",
        "Finally, we need a way evaluate the performance of the model. Without evaluation metrics we have no way to compare the performance of a given model, or compare the performance of several models.\n",
        "\n",
        "In the next sections, we will address each of loss functions, gradient computation and performance measurement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urU7pOG1WQl3"
      },
      "source": [
        "### 3.1 Loss functions\n",
        "\n",
        "To train a neural network we must have a **loss function**, also known as a **cost function**. In simple terms, the loss function measures the fit of a model to the training data. The lower the loss, the better the fit.\n",
        "\n",
        "To train deep learning models **cross entropy** is often used as a loss function. This is an information theoretic measure of model fit. We can understand cross entropy as follows.\n",
        "\n",
        "First define **Shannon entropy** as:\n",
        "\n",
        "$$\\mathbb{H}(I) = E[I(X)] = E[-ln_b(P(X))] = - \\sum_{i=1}^n P(x_i) ln_b(P(x_i)$$  \n",
        "Where:  \n",
        "$E[X] = $ the expectation of $X$.  \n",
        "$I(X) = $ the information content of $X$.   \n",
        "$P(X) = $ probability of $X$.  \n",
        "$b = $ base of the logarithm.    \n",
        "\n",
        "This rather abstract formula gives us a way to compute the expected information content of a set of values $X$. The more likely (higher probability) of $X$ the less informative it is.\n",
        "\n",
        "To create a loss function from the definition of Shannon entropy we start with the **Kullback-Leibler divergence (KL divergence)** or **relative entropy**. The KL divergence is an information theoretic measure of the difference between two distributions, $P(X)$ and $Q(X)$.\n",
        "\n",
        "$$\\mathbb{D}_{KL}(P \\parallel Q) = - \\sum_{i=1}^n p(x_i)\\ ln_b \\frac{p(x_i)}{q(x_i)}$$\n",
        "\n",
        "Ideally, in the case of training a machine learning model we want a distribution $Q(X)$, which is identical to the actual data distribution $P(X)$.\n",
        "\n",
        "But, you may say, if we could know $P(X)$ why compute $Q(X)$ at all? Fortunately, we do not have to. We can rewrite the KL divergence as:\n",
        "\n",
        "$$\\mathbb{D}_{KL}(P \\parallel Q) = \\sum_{i=1}^n p(x_i)\\ ln_b p(x_i) - \\sum_{i=1}^n p(x_i)\\ ln_b q(x_i)$$\n",
        "\n",
        "Since $P(X)$ is fixed and we wish to find $Q(X)$ when we train our model, we can minimize the term on the right, which is the **cross entropy** defined as:\n",
        "\n",
        "$$\\mathbb{H}(P,Q) = - \\sum_{i=1}^n p(x_i)\\ ln_b q(x_i)$$\n",
        "\n",
        "From the formulation of KL divergence above you can see the following.\n",
        "\n",
        "$$\\mathbb{D}_{KL}(P \\parallel Q) = \\mathbb{H}(P) + \\mathbb{H}(P,Q)\\\\\n",
        "\\mathbb{D}_{KL}(P \\parallel Q) = Entropy(P) + Cross\\ Entropy(P,Q)$$\n",
        "\n",
        "Thus, we can minimize divergence by minimizing cross entropy. This idea is both intuitive and computationally attractive. The closer the estimated distribution $q(X)$ is to the distribution of the true underling process $p(X)$, the lower the cross entropy and the lower the KL divergence.\n",
        "\n",
        "In general we will not know $p(X)$. In fact, if we did, why would we need to solve a training problem? So, we can use the following approximation.\n",
        "\n",
        "$$\\mathbb{H}(P,Q) = - \\frac{1}{N} \\sum_{i=1}^n ln_b q(x_i)$$\n",
        "\n",
        "You may notice, that this approximation, using the average log likelihood, is equivalent to a maximum likelihood estimator (MLE)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsxyMRgzWQl3"
      },
      "source": [
        "Let's look at a specific case of a model with Gaussian likelihood. What is the cross entropy? We can start by thinking about the definition of likelihood.\n",
        "\n",
        "$$p(data|model) = p(data|f(\\theta)) = p(x_i|f(\\hat{\\mu},\\sigma))= \\frac{1}{2 \\pi \\sigma^2} e^{\\frac{-(x_i - \\hat{\\mu})^2}{2 \\sigma^2}}$$\n",
        "\n",
        "We take the negative logarithm of this likelihood model.\n",
        "\n",
        "$$-log\\big(p(data|model) \\big) = - \\frac{1}{2}\\big( log( 2 \\pi \\sigma^2) + \\frac{(x_i - \\hat{\\mu})^2}{2 \\sigma^2} \\big)$$\n",
        "\n",
        "Now, the first term on the right is a constant, as is the denominator of the second term if we assume known variance. Since our goal is to minimize cross entropy, we can eliminate these quantities and be left with just the following.\n",
        "\n",
        "$$-(x_i - \\hat{\\mu})^2$$\n",
        "\n",
        "This is one issue we need to deal with. Our formulation of cross entropy involves the unknown true distribution of the underling process $p(X)$. However, since $p(x_i)$ is fixed but unknown we can just write the following.\n",
        "\n",
        "$$min \\big( \\mathbb{H}(P,Q) \\big) \\propto argmin_{\\mu} \\big( - \\sum_{i=1}^n (x_i - \\hat{\\mu})^2 \\big)$$\n",
        "\n",
        "This is just the definition of a Maximum Likelihood Estimator (MLE) for the least squares problem! In fact, since the cross entropy is computed using the negative log likelihood, it will always be minimized by the MLE.\n",
        "\n",
        "You can see another example of [cross-entropy error function and logistic regression](https://en.wikipedia.org/wiki/Cross_entropy)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eAM-LRrWQl3"
      },
      "source": [
        "### 3.2 Computing Loss functions\n",
        "\n",
        "The loss function is used to train the model. Therefore the loss function must be computed in an efficient manner.\n",
        "\n",
        "Given the number of parameters in deep neural nets over-fitting is inevitable. Therefore some regularization is required. We will discuss regularization in greater depth in another lesson. For now, we will just use the following regularized form.\n",
        "\n",
        "$$\\mathbb{H}(P,Q) = J(\\theta) = - \\frac{1}{N}\\sum_{i=1}^n ln_b q(x_i|\\theta) + \\lambda ||\\theta||^2\\\\\n",
        "where\\\\\n",
        "- \\frac{1}{N}\\sum_{i=1}^n ln_b q(x_i|\\theta) = J_{MLE}(\\theta)\\\\\n",
        "||\\theta||^2 = L^2\\ norm\\ regularization\\ term$$\n",
        "\n",
        "To minimize $J(\\theta)$ in this form $\\theta$ must be chosen to keep $||\\theta||^2$ small while minimizing the negative log likelihood of $q(x_i|\\theta)$.\n",
        "\n",
        "Let's  consider how we would compute this form of the lost function. The computational graph shown below illustrates the computational path for the regularize d loss function. For simplicity, no bias terms are considered.\n",
        "\n",
        "<img src=\"https://github.com/whendo/CSCI-E25/blob/assignment5/Labs/img/CompGraph2.jpg?raw=1\" alt=\"Drawing\" style=\"width:600px; height:300px\"/>\n",
        "<center>Figure 3.1  \n",
        "Computational graph for computing loss of fully connected neural network of Figure 2.3 </center>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg1hfVoTWQl3"
      },
      "source": [
        "### 3.3 Chain rule of calculus\n",
        "\n",
        "Key to the back propagation algorithm is the chain rule of calculus; not to be confused with the chain rule of probability. The chain rule allows us to back propagate gradients though an arbitrarily complex graph of functions.\n",
        "\n",
        "Now, suppose there is a function $y = g(x)$, and another function $z = f(y) = f(g(x))$. How do we compute the derivative of $z$ with respect to $x$? Applying the chain rule we get:\n",
        "\n",
        "$$\\frac{dz}{dx} = \\frac{dz}{dy}\\frac{dy}{dx}$$\n",
        "\n",
        "Consider $x \\in R^M$ $g(x) \\Rightarrow R^M$ and $ f(y) \\Rightarrow z \\in R$. The chain rule becomes:\n",
        "\n",
        "$$\\frac{\\partial z}{\\partial x} = \\sum_{j \\in M} \\frac{\\partial z}{\\partial y_j}\\frac{\\partial y_j}{\\partial x_i}$$\n",
        "\n",
        "Which we can rewrite as  \n",
        "\n",
        "$$\\nabla_{x}z = \\Big( \\frac{\\partial x}{\\partial y} \\Big)^T \\nabla_{y}z$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsgy-x-NWQl4"
      },
      "source": [
        "Here, $\\frac{\\partial x}{\\partial y}$ is the $n x m$ **Jacobian matrix** of partial derivatives. The Jacobian is multiplied by the gradient with respect to $y$, $\\nabla_{y}z$. You can think of the Jacobian as a transformation for a gradient with respect to $y$ to what we really want, the gradient with respect to $z$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMpGm1GRWQl4"
      },
      "source": [
        "### 3.4 Example of finding a gradient.\n",
        "\n",
        "Let's work out backpropagation for a very simple neural network with a just an input layer and an output layer. This neural network, including the loss function, is shown in Figure 3.2 below. This network has been highly simplified. There are only three layers, input layer, a two unit hidden layer with no bias terms, and a single unit output layer. There are only two weight tensors for this network. Further, the hidden units use rectilinear activation and the output unit uses linear activation. These activation functions have simple partial derivatives.  \n",
        "\n",
        "<img src=\"https://github.com/whendo/CSCI-E25/blob/assignment5/Labs/img/LossGraph.jpg?raw=1\" alt=\"Drawing\" style=\"width:600px; height:300px\"/>\n",
        "<center>Figure 3.2\n",
        "Simple single layer neural network with loss function </center>\n",
        "\n",
        "To analyze this network we will refer to the computational graph shown in Figure 3.1 above.\n",
        "\n",
        "First, we need to work out the forward propagation relationships. We can compute the outputs of the hidden layer as follows.\n",
        "\n",
        "$$S_{\\{1,2\\}} = \\sigma_h \\big( W^1 \\cdot X_{\\{1,2\\}} \\big) = \\sigma \\big( \\sum_j W^1_{i,j} x_j \\big)$$  \n",
        "\n",
        "In the same way, the result from the output layer can be computed as follows, since the activation function for this layer is linear.\n",
        "\n",
        "$$S_3 = W^2 \\cdot S_{\\{1,2\\}} = \\sum_i W^2_i \\sigma \\big( \\sum_j W^1_{i,j} x_j \\big)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jM7ua79iWQl4"
      },
      "source": [
        "To perform backpropagation, we need fill out the gradient vector by computing $\\frac{\\partial J(W)}{\\partial W}$ for each weight in the model.\n",
        "\n",
        "$$\\frac{\\partial J(W)}{\\partial W} =\n",
        "\\begin{bmatrix}\n",
        "\\frac{\\partial J(W)}{\\partial W^2_{11}} \\\\\n",
        "\\frac{\\partial J(W)}{\\partial W^2_{12}} \\\\\n",
        "\\frac{\\partial J(W)}{\\partial W^2_{21}} \\\\\n",
        "\\frac{\\partial J(W)}{\\partial W^2_{22}} \\\\\n",
        "\\frac{\\partial J(W)}{\\partial W^1_{1}} \\\\\n",
        "\\frac{\\partial J(W)}{\\partial W^1_{2}}\n",
        "\\end{bmatrix}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osgoIMPkWQl4"
      },
      "source": [
        "To keep things simple in this example we will just use a non-normalized squared error loss function. This is just the MLE estimator (without normalization) for a Gaussian distribution.\n",
        "\n",
        "$$J(W) = - \\frac{1}{2} \\sum_{l=1}^n (y_l - S_{3,l})^2 $$\n",
        "\n",
        "Where:  \n",
        "$y_k = $ the label for the lth case.     \n",
        "$\\hat{y_k} = S_{3,k} =$ the output of the network for the lth case.\n",
        "\n",
        "We want to compute the gradients with respect to the input and output tensors:\n",
        "\n",
        "$$\\frac{\\partial J(W)}{\\partial W^1}, \\ \\frac{\\partial J(W)}{\\partial W^2}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMG4HIwZWQl4"
      },
      "source": [
        "Let's start with the easier case of the partial derivatives with respect to the output tensor. We can apply the chain rule as follows:\n",
        "\n",
        "$$\\frac{\\partial J(W)}{\\partial W^2_k} = \\frac{\\partial  J(W)}{\\partial S_{3,k}} \\frac{\\partial S_{3,k}}{\\partial W^2_k}$$\n",
        "\n",
        "The first partial derivative of the chain is:\n",
        "\n",
        "$$\\frac{\\partial J(W)}{\\partial S_{3,k}} = \\frac{\\partial - \\frac{1}{2} (y_k - S_{3,k})^2} {\\partial S_{3,k}} = y_k - S_{3,k} $$\n",
        "\n",
        "And, the partial derivative of the second partial derivative in the chain, given the linear activation of the output unit:\n",
        "\n",
        "$$\\frac{\\partial S_{3,k}}{\\partial W^2_k} = \\frac{\\partial W^2_k S_{j,k}}{\\partial W^2_k}  = S_{j,l}, \\ j \\in \\{1,2\\}$$\n",
        "\n",
        "Multiplying the two components of the chain gives us:\n",
        "\n",
        "$$\\frac{\\partial J(W)}{\\partial W^2_k} = S_{j,k} (y_k - S_{3,k}), \\ j \\in \\{1,2\\} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JS2CNHW-WQl4"
      },
      "source": [
        "The partial derivatives with respect to the input tensor are a bit more complicated. To apply the chain rule we must work backwards from the loss function. This gives the following chain:\n",
        "\n",
        "$$\\frac{\\partial J(W)}{\\partial W^1_{i,j}} =  \\frac{\\partial J(W)}{\\partial S_{3}} \\frac{\\partial S_{3}}{\\partial S_{j}} \\frac{\\partial S_{j}}{\\partial W^1_{i,j}}$$\n",
        "\n",
        "First, we find the right most partial derivative in our chain:\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial S_j}{\\partial W^1_{i,j}} =\n",
        "\\begin{cases}\n",
        "     \\frac {\\partial W^1_{i,j} x_{i,k}}{\\partial W^1_{i,j}}, & \\text{if $S_j>0$} \\\\\n",
        "    0, & \\text{otherwise}\n",
        "  \\end{cases}\n",
        "\\end{equation}\n",
        "\n",
        "Which given the ReLU activation results in:\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial S_j}{\\partial W^1_{i,j}} =\n",
        "\\begin{cases}\n",
        "    1, & \\text{if $S_j>0$}  \\\\\n",
        "    0, & \\text{otherwise}\n",
        "  \\end{cases}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "The middle partial derivative must account for the nonlinearity:\n",
        "\n",
        "$$\\frac{\\partial S_{3}}{\\partial S_{j}} = W^2_j$$\n",
        "\n",
        "We have already computed $\\frac{\\partial J(W)}{\\partial S_{3}}$. Multiplying all three partial derivatives we find:\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial J(W)}{\\partial W^1_{i,j}} =\n",
        "\\begin{cases}\n",
        "    (y_k - S_{3,k}) W^2_j, & \\text{if $S_j>0$} \\\\\n",
        "    0, & \\text{otherwise}\n",
        "  \\end{cases}\n",
        "\\end{equation}\n",
        "\n",
        "Where $S_3$ and $S_{\\{1,2 \\}}$ are computed using the relationships given above.\n",
        "\n",
        "A more detailed, but still digestable example of computing gradients for backpropagation can be found in a blog post by [Manfred Zaharauskas](http://blog.manfredas.com/backpropagation-tutorial/), among many other places."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoIbz035WQl9"
      },
      "source": [
        "## 4.0 Creating a Model With Keras      \n",
        "\n",
        "You will now create and test a first deep learning classifier model for the MNIST dataset using Keras. The fully connected model has one hidden layer.            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MR68bSJqWQl9"
      },
      "source": [
        "### 4.1 Preparing the Dataset      \n",
        "\n",
        "You have already worked with the MNIST dataset. To load these data and prepare them for the Keras model execute the code in the cell below.      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "EpmS9ckMWQl9",
        "outputId": "8091427a-8bf5-408c-c1c5-7819965bdb92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "(60000, 784)\n",
            "(10000, 784)\n"
          ]
        }
      ],
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28*28)).astype('float32')/255\n",
        "print(train_images.shape)\n",
        "test_images = test_images.reshape((10000, 28*28)).astype('float32')/255\n",
        "print(test_images.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ALoXMfu1WQl9",
        "outputId": "152dbca9-d3a7-47e0-92a1-af3c63f13905",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('float32')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "train_images.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGnxErjCWQl9"
      },
      "source": [
        "There is one more preprocessing step. The labels need to be [**one hot encoded**](https://www.kaggle.com/dansbecker/using-categorical-data-with-one-hot-encoding). One hot encoding transforms an $N$ level categorical variable into $N$ binary columns. One column represents one category. A 1 or binary true value is encoded in the column of a given category with the other columns coded as 0 or false.          \n",
        "\n",
        "> **Exercise 5-3:** You will now one hot encoded the label vectors of the training and test data. Use the [keras.utils.np_utils.to_categorical](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical) function to create the one hot encoded labels. Print the first 10 rows of the training labels. You will need to set options to display all columns.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "scrolled": true,
        "id": "n_dFWJ5bWQl-",
        "outputId": "522757bc-f2ee-4b98-a305-2682a702d7fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 0 0 1 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 1]\n",
            " [0 0 1 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "## Put your code below\n",
        "train_labels = tf.keras.utils.to_categorical(\n",
        "    train_labels,\n",
        "    num_classes=None,\n",
        "    dtype='int32'\n",
        ")\n",
        "\n",
        "test_labels = tf.keras.utils.to_categorical(\n",
        "    test_labels,\n",
        "    num_classes=None,\n",
        "    dtype='int32'\n",
        ")\n",
        "\n",
        "print(train_labels[:10])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Cyo-zqwWQl-"
      },
      "source": [
        "> Examine the printed one hot encoded labels. Does the number of columns correspond to the number of label categories and why is this expected?   \n",
        "> **End of exercise.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c9r5_4ZWQl-"
      },
      "source": [
        "> **Answer:**  \n",
        "Yes, the number of label categories matches the number of columns in the one hot encoded matrix of labels. This is expected since each row is a unique combination of zeros and a one to reflect the number of options available in the label dataset.    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaO9RK61WQl-"
      },
      "source": [
        "## 4.2 Defining and Executing the Deep Learning Model\n",
        "\n",
        "It is now time to define the Keras neural network model. This model uses the [Keras **Sequential** class](https://keras.io/guides/sequential_model/). The model is constructed by **adding dense layers** with the `add` method. Hidden and output layers are specified by creating instances of the [Dense layer class](https://keras.io/api/layers/core_layers/dense/). An input data shape must be specified only for the input layer.         "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYXZJ6C5WQl-"
      },
      "source": [
        "> **Exercise 5-4:** You will now specify, [compile and fit](https://keras.io/api/models/model_training_apis/) the neural network model by the following steps:     \n",
        "> 1. Define the sequential model by instantiating a model object using [`models.Sequential`](https://keras.io/guides/sequential_model/). Name your model `nn`. Then add layers:  \n",
        ">   - Add a [dense input layer](https://keras.io/api/layers/core_layers/dense/) with $28 \\times 28$ hidden units, rectalinear (`relu`) activation and `input_shape=(28*28, )`.   \n",
        ">   - Add a dense hidden layer with 512 hidden units and rectalinear (`relu`) activation.     \n",
        ">   - Add a dense output output layer with `activation='softmax'`.     \n",
        "> 2. [Compile](https://keras.io/api/models/model_training_apis/) your model with the following arguments; `optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']`. We will discuss optimizers in another lesson.\n",
        "> 3. Print a summary of the model with the `summary()` method.\n",
        "> 3. Fit your model, using the [fit()](https://keras.io/api/models/model_training_apis/) method, with the training images, training labels, and arguments; `epochs=5, batch_size=128`.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "vUvgKNDPWQl-",
        "outputId": "8397ff46-aa84-4507-e4fb-7e856c589298",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_3 (Dense)             (None, 784)               615440    \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 512)               401920    \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 10)                5130      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1022490 (3.90 MB)\n",
            "Trainable params: 1022490 (3.90 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "469/469 [==============================] - 23s 45ms/step - loss: 0.2198 - accuracy: 0.9334\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 21s 45ms/step - loss: 0.0793 - accuracy: 0.9751\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 22s 48ms/step - loss: 0.0499 - accuracy: 0.9844\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 28s 59ms/step - loss: 0.0336 - accuracy: 0.9894\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 27s 57ms/step - loss: 0.0251 - accuracy: 0.9919\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ca592662110>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "# ## Your code goes here\n",
        "# Define the sequential model by instantiating a model object using models.Sequential. Name your model nn. Then add layers:\n",
        "nn = models.Sequential()\n",
        "# Add a dense input layer with  28×28  hidden units, rectalinear (relu) activation and input_shape=(28*28, ).\n",
        "nn.add(layers.Dense(28*28, activation='relu', input_shape=(28*28,)))\n",
        "# Add a dense hidden layer with 512 hidden units and rectalinear (relu) activation.\n",
        "nn.add(layers.Dense(512, activation='relu'))\n",
        "# Add a dense output output layer with activation='softmax'.\n",
        "nn.add(layers.Dense(10, activation='softmax'))\n",
        "# Compile your model with the following arguments; optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']. We will discuss optimizers in another lesson.\n",
        "nn.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# Print a summary of the model with the summary() method.\n",
        "nn.summary()\n",
        "# Fit your model, using the fit() method, with the training images, training labels, and arguments; epochs=5, batch_size=128.\n",
        "nn.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIh9VLoyWQl_"
      },
      "source": [
        "> Answer these questions:  \n",
        "> 1. Why is the `categorical_crossentropy` the good choice loss function and `softmax` the correct choice for output activation for this model?\n",
        "> 2. Considering the number of training samples, and number of trainable model parameters, where do you think this model might lie along the bias-variance trade-off spectrum?   \n",
        "> 2. Examine the evolution of the loss function and accuracy. What do these figures tell you about learning for this model?  \n",
        "> **End or exercise.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nWAvIgkWQl_"
      },
      "source": [
        "> **Answers:**\n",
        "> 1. Categorical Crossentropy is a good choice as a loss function because it's better as a multiple output loss function vs others better suited for binary or continuous outputs. Softmax is the correct choice for the output since it's also better for categorical data and is closely related to the categorical entropy loss function.    \n",
        "> 2. I would think that our model has low bias but high variance since we have a large number of training samples as well as a fairly large number of parameters, giving us a good chance to narrow down on our training data, but may cause issues with incoming test data that may differ some from the training set.    \n",
        "> 3.  One major effect of the loss function evolution is as the epochs increase, the accuracy seems to increase logrithmically, so we get far less benefite every time we run another epoch.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPmfpUQYWQl_"
      },
      "source": [
        "### 4.3 Performance Metrics\n",
        "\n",
        "Now that we have the components for training a basic neural network in place we need a way to evaluate its performance. It turns out, there is nothing special about evaluation of neural network models as opposed to other machine learning models. For regression models, one typically use the standard metrics such as root mean square error (RMSE), mean absolute error (MAE). For classification models, one also typically uses the standard metrics including the confusion matrix, accuracy, precision and recall. The [Keras metrics package](https://keras.io/api/metrics/) provides numerous methods for model evaluation.\n",
        "\n",
        "Execute the code in the cell below to compute and display performance metrics for your model based on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Hqi9meyxWQl_",
        "outputId": "8aa95238-a23f-43c3-f8f1-51b3e75d83de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 6s 16ms/step - loss: 0.0767 - accuracy: 0.9812\n",
            "Test accuracy = 0.981   Test loss = 0.077\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_accuracy = nn.evaluate(test_images, test_labels)\n",
        "print(\"Test accuracy = {0:.3f}   Test loss = {1:.3f}\".format(test_accuracy, test_loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NvND5UdWQl_"
      },
      "source": [
        "> **Exercise 5-5:** Compare the results of the evaluation with the same metrics achieved during model training. What does the difference tell you about the generalization for this model?  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfUd65KwWQl_"
      },
      "source": [
        "> **Answer:**\n",
        "The accuracy is slightly lower than it was with the training set, but not by much. This tells me the test set led to some generalization, but not enough to really hurt the model with this test set too much.      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vplz4x-yWQl_"
      },
      "source": [
        "## 5.0 Diagnosing Neural Network Training History   \n",
        "\n",
        "The Keras model `fit` method creates a [TensorFlow history object using callbacks](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/History). The information contained in the history object can be very useful in understanding model training; what is working and what is not.  \n",
        "\n",
        "Finding the a good set of hypterparameters for training any deep neural network can be a time consumming taks. **Do not expect your first choice of hypterparameters to be any good!**. Further, there is **no cut and paste from one problem to another.** Just because a particular set of hypterparamters resulted in a acceptable learning for one problem using one particular network does not mean that these hyperparameters will be suitable for any other problem or any other neural network.\n",
        "\n",
        "In the following exercises you will investigate a some commonly required steps to configure hypterparameters to allow a deep neural network toperform reasonable learning. The key to determining if a deep neural network is learning properly to study the evolution of the learning over the epochs. We want the curves of the evaluation metrics, such as loss and validation accuracy, to appear both smooth and with slowly changing slope. Once the model has been trained for a sufficient number of epochs, the metric curves should flatten, having essentially zero slope. The zero slope indicates the learning has ended. If the metric curves appear erratic, you know that the learning is not proceeding smoothly and is too agressive. If the curves have abrupt changes in slope this indicates the learning is too agressive as well. For example, a curve that has a sharp knee and then near zero slope can indicate that learning terminated prematurely.    \n",
        "\n",
        "We will apply the following methods to improve the learning of deep neural network models:        \n",
        "1. **Regularization** is necessary to ensure proper training of deep neural networks. If the model has insufficient regularization, the learning will be erratic with large jumps in the validation metrics, such as error rate and accuracy. On the other hand if too much regularization is applied, learning will be limited by the high bias in the model.    \n",
        "2. **Learning rate** must be set so that learning proceeds slowly and steadily. If the learning is too fast, the learning will terminate prematurely. The, typically local, solution for the weight parameters will be far from optimal. You can identify such cases by a steep curve that flattens out for the performance metrics. In other cases, the agressive learning will result in erratic curves of the learning metrics. However, if the learning is too slow the neural network will take an excessive number of epochs to reach a steady state where learning has ceased. You can identify such cases by metric curves that never really converge to a zero (flat) slope. However, keep in mind that with complex networks which require learning many parameters, a large number of training epochs may be required.      \n",
        "3. Generally, there is an **interaction between regularization and learning rate**. Changing hyperparamters for regulaizers can affect the optimal larning rate and vice versa.   \n",
        "\n",
        "Let's put these concepts into practice.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkIK9qwkWQmA"
      },
      "source": [
        "> **Exercise 5-6:** You will now re-train your model while capturing a history. To retrain a model you must re-compile it first to create a fresh model object. Training an existing model object will continue the training of that object. This property of Keras models can be most useful for improving existing models when new training data becomes available. Do the following:      \n",
        "> 1. Compile the existing model using the same arguments as before.     \n",
        "> 2. Fit the model as before, but for 20 epochs and with an additional argument; `validation_data=(test_images, test_labels)`. Assign the results to a variable, `history_nn`.   \n",
        "> 3. Execute the code in the next two cells to display charts of training and test loss and accuracy.     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0pLSGMCWQmA",
        "outputId": "d7bd377a-ebb3-4d72-882a-502b56d11e20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "469/469 [==============================] - 26s 54ms/step - loss: 0.0176 - accuracy: 0.9944 - val_loss: 0.0693 - val_accuracy: 0.9807\n",
            "Epoch 2/20\n",
            "168/469 [=========>....................] - ETA: 14s - loss: 0.0111 - accuracy: 0.9964"
          ]
        }
      ],
      "source": [
        "## Your code goes here\n",
        "# Compile the existing model using the same arguments as before.\n",
        "nn.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# Fit the model as before, but for 20 epochs and with an additional argument; validation_data=(test_images, test_labels). Assign the results to a variable, history_nn.\n",
        "history_nn = nn.fit(train_images, train_labels, epochs=20, batch_size=128, validation_data=(test_images, test_labels))\n",
        "# Execute the code in the next two cells to display charts of training and test loss and accuracy.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFIVkDIYWQmA"
      },
      "source": [
        "> Next, execute the code in the cell below to plot the training and validation loss and accuracy histories.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYo-D7jzWQmA"
      },
      "outputs": [],
      "source": [
        "def plot_loss(history):\n",
        "    train_loss = history.history['loss']\n",
        "    test_loss = history.history['val_loss']\n",
        "    x = list(range(1, len(test_loss) + 1))\n",
        "    plt.plot(x, test_loss, color = 'red', label = 'test loss')\n",
        "    plt.plot(x, train_loss, label = 'traning loss')\n",
        "    plt.yscale('log')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Log Loss')\n",
        "    plt.title('Loss vs. Epoch')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_accuracy(history):\n",
        "    train_acc = history.history['accuracy']\n",
        "    test_acc = history.history['val_accuracy']\n",
        "    x = list(range(1, len(test_acc) + 1))\n",
        "    plt.plot(x, test_acc, color = 'red', label = 'test accuracy')\n",
        "    plt.plot(x, train_acc, label = 'training accuracy')\n",
        "    plt.yscale('log')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Log Accuracy')\n",
        "    plt.title('Accuracy vs. Epoch')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "plot_loss(history_nn)\n",
        "plot_accuracy(history_nn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2k1cRCDWQmA"
      },
      "source": [
        "> Examine the trajectory of the training and test loss and accuracy. What does the erratic and divergent trajectories of the loss and accuracy tell you about the learning and generalization of the model?       \n",
        "> **End of exercise.**    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hk0l_sHhWQmA"
      },
      "source": [
        "> **Answer:**      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDjQ9VOOWQmA"
      },
      "source": [
        "## 6.0 Adding Regularization        \n",
        "\n",
        "You have seen some of the effects of over-fitting of the neural network model. **Regularization** methods are widely used in machine learning to prevent over-fitting. Conceptually, you can think of regularization as moving the model toward lower variance and higher bias to improve generalization. There are several widely used regularizaiton methods for deep learning models. These methods are generally used in combinations.     \n",
        "- **L1 and L2 regularization** which we examined in some detail in the previous lesson.\n",
        "- **Weight decay** exponentially decays the learned weights toward 0. This process constrains the learning and prevents over-fitting. Any unusually large weights will decay back toward zero and therefore more reasonable values.          \n",
        "- **Batch Normalization** adjusts the output of a layer to ensure it is zero mean and unit variance. This process ensures that the results of the activations of a layer are not excessive that therefore result in over-fitting.         \n",
        "- **Dropout regularization** randomly drops connections (weights) between layers as the learning proceeds. The resulting model is then an ensemble of the weights learned from each of the randomly created models. This ensemble improves regularization and generalization of the model.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMZ_yBM5WQmB"
      },
      "source": [
        "With so many options a search of possible regularization methods and hyperparamters is required. This search requires running a particular neural network architecture multiple times as hyperparameter combinations are tested. There are a number of approaches one can take to this search.      \n",
        "- **Grid search** wherein a regular grid of hyperparameter choices are tried. Grid search has the advantage of being comprehensive, but at a high computational cost.     \n",
        "- **Random sampling** wherein a grid of possible hyperparameter choices is randomly sampled and tested. This method can be significantly faster than a grid search, and is often nearly as effective.     \n",
        "- **Huristic search** is often performed manually, selecting hyperparamter combinations based on the results of previous combinations. We will employ huristic search in these exercises because of the simplicity.    \n",
        "\n",
        "In the case used for the following exercise, the following hyperparameter combinations where tried. Keep in mind, the exact validation metrics achieved will vary from one run to the next as a result of random sampling effects.\n",
        "\n",
        "| Validatiion Accuracy | L2 regularization | Learning rate | Weight decay | Batch normalization |   \n",
        "| -------------------- | ----------------- | ------------- | ------------ | ---------------------- |\n",
        "| .8979                | 0.05              |  0.00005      | 0.0001       |  None                  |\n",
        "| .8754               | 0.5              |  0.000005      | None      |  None                  |\n",
        "| .8763                | 0.05              |  0.000005      | 0.0001       |  None                  |\n",
        "| Erratic learning      | 0.005              |  0.0005      | 0.0001       |  None                  |\n",
        "| .9462               | 0.005              |  0.0005      | 0.01       |  None                  |\n",
        "\n",
        "Notice that the hypterparameters control several aspects of the model training.     \n",
        "1. The L2 affects the weights learned for the [Dense](https://keras.io/api/layers/core_layers/dense/) layer. Notice that regularizers can be applied to other weights, such as the bias. We will ignore these alternatives for now.     \n",
        "2. The momentum hypterparameter is the primary control for the behavior of the [BatchNormalization](https://keras.io/api/layers/normalization_layers/batch_normalization/) layer.      \n",
        "3. The [RMSprop](https://keras.io/api/optimizers/rmsprop/) optimizer has quite a number of hypterparameters. The learning rate and weight decay are the most important in general. Selecting an optimal value of momentum can be helpful in some cases. Other hypterparameters are helpful for cases where the optimizer fails to converge. It should be noted that it is not unusual for a particular choice of optimizer to not work well for a specific problem and neural network. Another optimizer can then be tried. Keras, like most deep learning frameworks, has a [long list of optimizers](https://keras.io/api/optimizers/) you can choose from.   \n",
        "\n",
        "> **Important note:** In the following examples we limit the search for optimal hyperparamters in several ways.\n",
        "> 1. We will only search a limited number of architectural alternatives. In this case, we will test models with and without batch normalization, a form of regularization. For a real-world problem, a more extensive search for optimal model architecure is often required. For example, we could try greater model depth, in the form of more fully connected layers, or we could experiment with adding dropout regularization layers.      \n",
        "> 2. We will only search a limited hyperparameter space. A full search would take considerable computing time, but could be worth the improvement if we were optimizing a model for real-world use.    \n",
        "> 3. We are limiting training time to just 80 epochs. For complex models it is typical to train for hundreds or even thousands of epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44NLzn1SWQmB"
      },
      "source": [
        "> **Exercise 5-7:** You will now add L2 combined with weight decay regularization to the hidden layers of your model. Keras uses the [layer weight regularizer class](https://keras.io/api/layers/regularizers/) to add L1 and L2 weight constraints to the to layers. Here, we will only used the L2 regularizer. At the same time we must change the learing rate. Do the following:    \n",
        "> 1. Starting with the model specification you have been using for exercise 5-6, add the following argument to the input and hidden layer specificaition; `kernel_regularizer=regularizers.l2(0.005)` argument.    \n",
        "> 2. Compile the model with the 'optimizer=RMSprop(learning_rate=0.0005, decay=0.01)' argument. This code sets a learning rate and a weight decay parameter.  \n",
        "> 3. Fit the model for 80 epochs, saving the history object. This will take some time!   \n",
        "> 4. Plot the training and test loss and accuracy.\n",
        "\n",
        "> **Note on compatibility.**  Depending on the version of TensorFlow you are using the `decay` argument may acually be `weight_decay`. If you find an error of an unknown argument try changing to the other possibility.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYglDIaUWQmB"
      },
      "outputs": [],
      "source": [
        "## Your code goes here\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmQbyFtPWQmB"
      },
      "source": [
        "> Compare the charts of loss and accuracy for the regularized model with those from training the unregularized model. The shape of the training and validation metric curves is greatly improved. Now, answer these questions:    \n",
        "> 1. Do the charts and numeric metrics for the regularized model show an improvement in the generalization of the model compared to the unregularized model, and why?      \n",
        "> 2. Notice that the test and training loss of the regularized model continue to improve. Do you think that training for additional epochs will be beneficial?       \n",
        "> 3. Do the validation metrics show any sign of randomness and what does this tell you about the model architecture and the choice of hyperparameters?   \n",
        "> **End of exercise.**      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKhkvfUeWQmB"
      },
      "source": [
        "> **Answers:**    \n",
        "> 1.     \n",
        "> 2.    \n",
        "> 3.    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RUgjm2KWQmC"
      },
      "source": [
        "As a next step to improve the model we will add a batch normalization layer following the fully connected hidden layer. As a result of this change in architecture, we must perform another hyperparameter search. In this case, another huristic search is performed with a fixed momentum hyperparameter for the batch normalization.The results of the search are shown in the table.        \n",
        "\n",
        "| Validatiion Accuracy | L2 regularization | Learning rate | Weight decay | Batch normalization |   \n",
        "| -------------------- | ----------------- | ------------- | ------------ | ---------------------- |\n",
        "| .9752            | 0.2             |  0.00005      | None     |  Momentum=0.998                 |\n",
        "|Erratic learning  | 0.2             |  0.0005      | 0.001       |  Momentum=0.998                 |\n",
        "|Slow learning    | 0.02             |  0.00005      | 0.01       |  Momentum=0.998                 |\n",
        "|Slow learning    | 0.02             |  0.00005      | 0.01       |  Momentum=0.998                 |\n",
        "|.9789             | 0.2             |  0.0005      | 0.01       |  Momentum=0.998                 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTt6h6SnWQmC"
      },
      "source": [
        "> **Exercise 5-8:** The path of the test loss and particularly the test accuracy of the foregoing model is rather erratic. Such behavior often indicates that the gradient is diffcult to estimate and therefore the optimizer exhibits poor convergence. Batch normalizaiton is known to smooth the loss function which improves the ability to consistently estimate gradient. You will now add a **[BatchNormalization layer](https://keras.io/api/layers/normalization_layers/batch_normalization/)** to the model, along with L2 regularization. Do the following:    \n",
        "> 1. Start with the model specification you used for Exercise 5-7, change the model name to `nnbr`.\n",
        "> 2. For the dense hidden layer set the L2 regularization hypterparameter to 0.2.  \n",
        "> 3. After the 512 unit hidden dense layer add a [BatchNormalization](https://keras.io/api/layers/normalization_layers/batch_normalization/) layer, using the argument `momentum=0.998`.\n",
        "> 4. Compile the model with the optimizer=RMSprop(learning_rate=0.0005, decay=0.01), argument.   \n",
        "> 5. Fit the model for 80 epochs, saving the history object. This will take some time!   \n",
        "> 6. Plot the training and test loss and accuracy.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQ66Jbc9WQmC"
      },
      "outputs": [],
      "source": [
        "## Your code goes here\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swjoC_F4WQmC"
      },
      "source": [
        "> Compare the graphs to the train and test loss and train and test accuracy to the foregoing model without the batch normalization and answer these questions:  \n",
        "> 1. What does the difference in smoothness, especially for the test accruacy, tell you about the change in behavior of the gradient?     \n",
        "> 2. Notice the overall shape of the learning metric curves. What might the abrupt change in slope of the validation accuracy curve tell you about the learning rate?        \n",
        "> **End of exercise.**   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgOQQUYqWQmD"
      },
      "source": [
        "> **Answers:**\n",
        "> 1.      \n",
        "> 2.     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmXUN68NWQmD"
      },
      "source": [
        "## 7.0 Learning Rate Decay  \n",
        "\n",
        "The foregoing model training appears to have worked rather well. But, can we do better? One possibility is to use a **adaptive learning rate** or **decaying learning rate**. Keras, like most deep learning frameworks, provides [utilities to define decaying learning rate schedules](https://keras.io/api/optimizers/learning_rate_schedules/). Here we will work with an exponential learning rate. You can see specific examples of using this utility [here](https://keras.io/api/optimizers/learning_rate_schedules/exponential_decay/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDPE0ZhQWQmD"
      },
      "source": [
        "| Validatiion Accuracy | L2 regularization | Learning rate | Weight decay | Batch normalization |   \n",
        "| -------------------- | ----------------- | ------------- | ------------ | ---------------------- |\n",
        "| .9810               | 0.2              |  initial rate=5e-5      | None      |  Momentum=0.998       |\n",
        "| Erratic results     | 0.2              |  initial rate=5e-4      | None      |  Momentum=0.998       |\n",
        "| Very slow learning  | 0.2              |  initial rate=5e-6      | None      |  Momentum=0.998       |\n",
        "| .9225              | 0.02              |  initial rate=5e-5      | 0.01      |  Momentum=0.998       |\n",
        "| .9258              | 0.002              |  initial rate=5e-5      | 0.01      |  Momentum=0.998       |\n",
        "| .9640            | 0.002              |  initial rate=5e-5      | 0.001      |  Momentum=0.998       |\n",
        "| .9644           | 0.0002              |  initial rate=5e-5      | 0.001      |  Momentum=0.998       |\n",
        "| .9779           | 0.0002              |  initial rate=5e-5      | 0.0001      |  Momentum=0.998       |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt3t_MqHWQmD"
      },
      "source": [
        "> **Exercise 5-9:**  You will now implement and test a model with an exponentially decreasing learning rate:    \n",
        "> 1. Start with the model specification you used for Exercise 5-8, change the model name to `nnvr`.\n",
        "> 2. For the dense hidden layer set the L2 regularization hypterparameter to 0.2.  \n",
        "> 3. Create a learning rate schedule using the [Keras exponential learning rate scheduler] (https://keras.io/api/optimizers/learning_rate_schedules/exponential_decay/) with 'initial_learning_rate=5e-5', 'decay_steps=1000', 'decay_rate=0.9'.\n",
        "> 4. Compile the model with the optimizer=RMSprop(learning_rate=your_lerarning_rate_schedule), argument.   \n",
        "> 5. Fit the model for 80 epochs, saving the history object. This will take some time as before.   \n",
        "> 6. Plot the training and test loss and accuracy.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgKUklyvWQmD"
      },
      "outputs": [],
      "source": [
        "## Your code goes here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wD9st7eWQmD"
      },
      "source": [
        "> Examine the results and answer these questions:       \n",
        "> 1. Compare the validation accuracy curve and final accuracy to the same curve and final accuracy value for the model trained with fixed learning rate. What do the differences in the curve and final learning rate tell you about the effect of using a variable learning rate?      \n",
        "> 2. Examine the shape of the training and validation curves. What does this shape tell you about the learning of this model?        \n",
        "> **End of exercise.**   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIwAerwdWQmE"
      },
      "source": [
        "> **Answers:**       \n",
        "> 1.    \n",
        "> 2.    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vPhh5_dWQmE"
      },
      "source": [
        "## 8.0 Dropout Regularization   \n",
        "\n",
        "Dropout regularization randomly drops (set to zero weight) connections in the neural network. The fraction of weights dropped is a hyperparameter which must be set. A good typical starting values is around 0.5. Emerical evidence indicates that results are not particularly sensitive to this value.      \n",
        "\n",
        "Here, the batch normalization layer has been replaced by a dropout regularization layer. This seemingly small change actually has a large effect on the model architecture. As a result, an extensive hyperparameter search was required to achieve good model performance. The hyperparameter choices tried are summarized in the table below.        \n",
        "\n",
        "You many wonder why the final model selected has lower accuacy than some other hyperparameter choices. These seemingly superior choices resulted in very odd convergence behavior of the model. In these cases the training metrics continued to improve considerably, whereas the validation metrics where stuck at a significantly different level. This outcome indicated that the model was in overfit or the training was stuck at some local optimum. To correct this problem, additional mild regularizaton was added. The regularization added bias to the model training, reducing the accuacy metric slightly, but corrected the problem of serious over-fitting.    \n",
        "\n",
        "> **Important note:** For the simplicity of our examples, we have substituted the dropout regularization layer for the batch normalization layer. You should not conclude from this example that one cannot or should not use both types of regularizers. In fact, in many deep architectures using both dropout regularization and batch normalization together is common practice.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S-OeLN9WQmE"
      },
      "source": [
        "| Validatiion Accuracy | L2 regularization | Learning rate | Weight decay | Dropout rate |   \n",
        "| -------------------- | ----------------- | ------------- | ------------ | ---------------------- |\n",
        "| Very slow learning    | 0.2              |  initial rate=5e-5      | None      |  0.5       |\n",
        "|  Very slow learning     | 0.02              |  initial rate=5e-5      | None      |  0.5       |\n",
        "|  Very slow learning  | None             |  initial rate=5e-5      | 0.01      |  0.5       |\n",
        "| .9525   | None             |  initial rate=5e-5      | 0.0001   |  0.5       |\n",
        "| .9620  | None             |  initial rate=5e-5      | none   |  0.5       |\n",
        "| .9666  | None             |  initial rate=5e-5      | none   |  0.3       |\n",
        "| .9835, divergence between train and validation | None             |  initial rate=5e-4      | none   |  0.3       |\n",
        "| .9832, divergence between train and validation | None             |  initial rate=5e-4      | none   |  0.5       |\n",
        "| .9838, divergence between train and validation | None             |  initial rate=5e-4      | none   |  0.1       |\n",
        "| .9822, less divergence | 0.0002             |  inittheial rate=5e-4      | 0.0001  |  0.1       |\n",
        "| .9814 | 0.0002             |  initial rate=5e-4      | 0.0001   |  0.1       |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bufASzV5WQmE"
      },
      "source": [
        "> **Exercise 5-10:**  You will now create and evaluate a model using a dropout regularization layer:   \n",
        "> 1. Start with the model specification you used for Exercise 5-9, change the model name to `nndo`.\n",
        "> 2. For the dense hidden layer set the L2 regularization hypterparameter to 0.0002.  \n",
        "> 3. Replace the batch normalization layer with a [Dropout regularization layer](https://keras.io/api/layers/regularization_layers/dropout/) with `rate=0.5`.\n",
        "> 4. Create a learning rate schedule using the [Keras exponential learning rate scheduler] (https://keras.io/api/optimizers/learning_rate_schedules/exponential_decay/) with 'initial_learning_rate=5e-4', 'decay_steps=1000', 'decay_rate=0.9'.\n",
        "> 4. Compile the model with the optimizer=RMSprop(learning_rate=lr_schedule, decay=0.0001), argument.   \n",
        "> 5. Fit the model for 80 epochs, saving the history object. This will take some time as before.   \n",
        "> 6. Plot the training and test loss and accuracy.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Vy79azgWQmF"
      },
      "outputs": [],
      "source": [
        "## Your code goes here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WmKQrO0WQmF"
      },
      "source": [
        "> Examine the accuracy value achived and the curves for the training and validation metrics and answer these questions.      \n",
        "> 1. How do the final trajectory of the validation accuracy curve and validation accuracy compare to the perviously fit model using batch normalizatuion?                    \n",
        "> 2. Examine the change in slope in this curve, comparing it to the perviously fit model using batch normalizatuion. What might the difference in the shapes tell you about the learning of this new model?        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCKfuJKoWQmF"
      },
      "source": [
        "> **Answers:**      \n",
        "> 1.       \n",
        "> 2.    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2VRiVNbWQmF"
      },
      "source": [
        "##### Copyright 2018, 2019, 2020, 2021, 2022, 2023, 2024, Stephen F Elston. All rights reserved"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_OfsbyeWQmF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}