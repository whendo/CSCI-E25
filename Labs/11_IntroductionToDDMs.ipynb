{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "827f432e-ff64-467e-a648-a124a03e6fc4",
   "metadata": {},
   "source": [
    "# Introduction to Diffusion Generative Networks\n",
    "\n",
    "## Stephen Elston\n",
    "\n",
    "## Overview\n",
    "\n",
    "Though these exercises you will gain some basic understanding of **denoising deffusion implicit models (DDIMs)**. DDIMs are image generative models which synthesize high quality images.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c928db3-3443-4e2a-9e4e-687017305e6d",
   "metadata": {},
   "source": [
    "## Running the Example Code     \n",
    "\n",
    "For the exercises you will first need to run the code in this [Keras example notebook by Andres Beres](https://keras.io/examples/generative/ddim/). To run the notebook you will need a [Google Colabratory account](https://colab.research.google.com/) if you do not already have one. Log into your google account. From the Keras example page click the `View in Colab` tab to start the notebook in Colab (but do not execute yet!). Alternatively, you may want to run this notebook locally, if you have the resources.     \n",
    "\n",
    "On several tests with Colab and Colab Pro+ this notebook executed in under 2 hours with no time outs. If you enconter problems, please reach out to the instructional staff.   \n",
    "\n",
    "There are two small changes you will need to make in this notebook.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c212c7d1-a844-4c73-b610-1f3693f5b132",
   "metadata": {},
   "source": [
    "### Update Keras and TensorFlow\n",
    "\n",
    "You will likely find it necessary to update the versions of Keras and TensorFlow so the Keras ops package will install correctly. Create a code cell before the imports, add the code shown below and execute it. You will then need to restart the Kernel to apply the update.    \n",
    "> **Note:** You will see some warnings messages, which can usually be ignored in this case.  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "c04435ef-08a8-4821-85b7-5c4f1e280507",
   "metadata": {},
   "source": [
    "!pip install tensorflow==2.16.1\n",
    "!pip install keras==3.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc06513-7464-43d6-b21e-566325303b5b",
   "metadata": {},
   "source": [
    "### Set the Number of Traing Epochs    \n",
    "\n",
    "As provided in this notebook, the code will only train the model for 1 epoch. The author recommends using 50 epochs to produce quality images. However, experiments show that the series of images displayed from the history will overflow the TensorFlow history buffer. Therefore, we will only train for 25 epochs, which will produce images of limited but still reasonable quality.  \n",
    "\n",
    "To train for 25 epochs change the third line in the Hypterpareters code block to the following.   "
   ]
  },
  {
   "cell_type": "raw",
   "id": "715ed0a8-43cf-4f46-b853-7ff3967668f0",
   "metadata": {},
   "source": [
    "num_epochs = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277ea94e-1192-4e2b-af31-26dce65bf2e1",
   "metadata": {},
   "source": [
    "### Configuring Runtime\n",
    "\n",
    "It is recommended that you configure your at either of:     \n",
    "- T4 GPU, High-RAM       \n",
    "- V100 GPU, High-RAM     \n",
    "\n",
    "You can see your current runtime environment in the upper right corner of the Colab screen.      \n",
    "\n",
    "<img src=\"../img/ColabRuntime.png\" alt=\"Drawing\" style=\"width:500px; height:150px\"/>\n",
    "<center>Display of the Colab runtime environment</center>     \n",
    "\n",
    "If you wish to change the runtime environment, use the pulldown to display this box, from which you can select your runtime.   \n",
    "\n",
    "<img src=\"../img/ColabChangeRuntime.png\" alt=\"Drawing\" style=\"width:500px; height:300px\"/>\n",
    "<center>Box for configuring Colab runtime environment</center>     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186cf020-d019-4bf2-873b-c8021fbbe60c",
   "metadata": {},
   "source": [
    "## Exercises     \n",
    "\n",
    "With the notebook updated carefully read the explainatory text provided by the author. Then, execute the code and answer the questions in the following exercises.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498ff711-6e54-4bc5-827d-277f40a211c8",
   "metadata": {},
   "source": [
    "> **Exercise 11-1:** In the DiffusionModel class examine the code in the diffusion_schedule method. Also, read the discussion in the diffusion schedule subsection of the text. What can you say about the rate of the increase of the added noise, $\\epsilon_t$, in the steps of the forward diffusion process?                  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d5eb0d-6796-4703-a1bf-4905ec8c1ef9",
   "metadata": {},
   "source": [
    "> **Answer:**    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b85faf4-f4ef-4ea6-9546-ce7b29f8e438",
   "metadata": {},
   "source": [
    "> **Exercise 11-2::** The code shown below is copied from the train_step method of the ReverseDiffusion object.   "
   ]
  },
  {
   "cell_type": "raw",
   "id": "dafd29c1-cbb4-4e1e-a7a3-f8f584a0c200",
   "metadata": {},
   "source": [
    "        # sample uniform random diffusion times\n",
    "        diffusion_times = keras.random.uniform(\n",
    "            shape=(batch_size, 1, 1, 1), minval=0.0, maxval=1.0\n",
    "        )\n",
    "        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
    "        # mix the images with noises accordingly\n",
    "        noisy_images = signal_rates * images + noise_rates * noises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aa7761-6be9-437b-9c6f-57926889808a",
   "metadata": {},
   "source": [
    "> In a few sentances, explain how this code implements the reverse steps of the random diffusion process?           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5997b2b6-67ff-486b-938a-c13858bc969e",
   "metadata": {},
   "source": [
    "> **Answer:**     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f38ec8b-3145-433b-b8eb-697cf4eda438",
   "metadata": {},
   "source": [
    "> **Exercise 11-3:** The code below was copied from the test_step method of the ReverseDiffusion object. The code calls the denoise method which uses the trained UNET neural network            "
   ]
  },
  {
   "cell_type": "raw",
   "id": "45fd6789-0f2c-4d32-8b55-12d466f75b3d",
   "metadata": {},
   "source": [
    "pred_noises, pred_images = self.denoise(\n",
    "            noisy_images, noise_rates, signal_rates, training=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bde60e3-a1d4-4a03-8176-814f358bd4a1",
   "metadata": {},
   "source": [
    "> In one or a few sentances explain what the results returned from the denoise method mean in terms of the reverse diffusion process and what the neural network has learned?      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a539998-baf8-40b2-aa30-59abeffd72e5",
   "metadata": {},
   "source": [
    "> **Answer:**    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0340bdea-eada-4087-a04a-b0374a31a1c1",
   "metadata": {},
   "source": [
    "> **Exercise 11-4:** Examine the squence of generated images produced in the history of the training epochs.These images represent the end product of the denoising Markov chain.\n",
    "> 1. What do the images produced in the first few epochs tell you about the learning of the denoising process parameters?\n",
    "> 2. Given the quality of the images in the laast few epochs what can you say about the neural network's ability to learn the noise parameters, $\\epsilon_\\theta$?      \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a33d0c0-016e-4a38-918c-f6c3088f156c",
   "metadata": {},
   "source": [
    "> **Answers:**        \n",
    "> 1.      \n",
    "> 2.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f044490-55b3-406b-9aa1-72a23a4b436d",
   "metadata": {},
   "source": [
    "> **Exercise 11-5:**  Examine the images produced by the best trained model in the Inference section of the notebook. All of the images are of floweres. However, there is no guidance indicating the images produced should be of flowers.    \n",
    "> 1. Considering the training images, why do you think only flower images are crested?\n",
    "> 2. What are the implications of your answer to the above question in terms the distributions required for creating a diffusion image generative model that can create a wide range of images?\n",
    "> 3. What are the limitations you have just discussed have for the general ability of generative diffusion models to produce arbitrary iamges in practice?       \n",
    "> **End of exercise.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651f01ad-9ea8-4b01-b8f3-361d9f02a629",
   "metadata": {},
   "source": [
    "> **Answers:**\n",
    "> 1.     \n",
    "> 2.     \n",
    "> 3.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456bdb12-6b5f-4a6c-a607-a660689c0566",
   "metadata": {},
   "source": [
    "#### Copyright 2024, Stephen F. Elston. All rights reserved.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
