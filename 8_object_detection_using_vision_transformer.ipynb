{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whendo/CSCI-E25/blob/master/8_object_detection_using_vision_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMu8O6qmkhHA"
      },
      "source": [
        "# Object detection with Vision Transformers\n",
        "\n",
        "**Author:** [Karan V. Dave](https://www.linkedin.com/in/karan-dave-811413164/)<br>\n",
        "**Date created:** 2022/03/27<br>\n",
        "**Last modified:** 2023/11/20<br>\n",
        "**Description:** A simple Keras implementation of object detection using Vision Transformers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIQ2cUgwkhHC"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "The article\n",
        "[Vision Transformer (ViT)](https://arxiv.org/abs/2010.11929)\n",
        "architecture by Alexey Dosovitskiy et al.\n",
        "demonstrates that a pure transformer applied directly to sequences of image\n",
        "patches can perform well on object detection tasks.\n",
        "\n",
        "In this Keras example, we implement an object detection ViT\n",
        "and we train it on the\n",
        "[Caltech 101 dataset](http://www.vision.caltech.edu/datasets/)\n",
        "to detect an airplane in the given image."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow --upgrade\n",
        "!pip install keras --upgrade"
      ],
      "metadata": {
        "id": "W70TU-3Ek5w6",
        "outputId": "a55f4249-65b2-4b04-d15d-88e021209fc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Collecting h5py>=3.10.0 (from tensorflow)\n",
            "  Downloading h5py-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow)\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.10.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.1)\n",
            "Collecting tensorboard<2.17,>=2.16 (from tensorflow)\n",
            "  Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras>=3.0.0 (from tensorflow)\n",
            "  Downloading keras-3.1.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
            "Collecting namex (from keras>=3.0.0->tensorflow)\n",
            "  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
            "Collecting optree (from keras>=3.0.0->tensorflow)\n",
            "  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n",
            "Installing collected packages: namex, optree, ml-dtypes, h5py, tensorboard, keras, tensorflow\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.2.0\n",
            "    Uninstalling ml-dtypes-0.2.0:\n",
            "      Successfully uninstalled ml-dtypes-0.2.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.9.0\n",
            "    Uninstalling h5py-3.9.0:\n",
            "      Successfully uninstalled h5py-3.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.16.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h5py-3.10.0 keras-3.1.1 ml-dtypes-0.3.2 namex-0.0.7 optree-0.11.0 tensorboard-2.16.2 tensorflow-2.16.1\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.1.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.25.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.7)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.10.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.11.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.3.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.10.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZGzlsELkhHD"
      },
      "source": [
        "## Imports and setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkNwlxFnkhHD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # @param [\"tensorflow\", \"jax\", \"torch\"]\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras import ops\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import scipy.io\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVdeaFJLkhHE"
      },
      "source": [
        "## Prepare dataset\n",
        "\n",
        "We use the [Caltech 101 Dataset](https://data.caltech.edu/records/mzrjq-6wc02)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cp_CAz1khHE"
      },
      "outputs": [],
      "source": [
        "# Path to images and annotations\n",
        "path_images = \"./101_ObjectCategories/airplanes/\"\n",
        "path_annot = \"./Annotations/Airplanes_Side_2/\"\n",
        "\n",
        "path_to_downloaded_file = keras.utils.get_file(\n",
        "    fname=\"caltech_101_zipped\",\n",
        "    origin=\"https://data.caltech.edu/records/mzrjq-6wc02/files/caltech-101.zip\",\n",
        "    extract=True,\n",
        "    archive_format=\"zip\",  # downloaded file format\n",
        "    cache_dir=\"/\",  # cache and extract in current directory\n",
        ")\n",
        "download_base_dir = os.path.dirname(path_to_downloaded_file)\n",
        "\n",
        "# Extracting tar files found inside main zip file\n",
        "shutil.unpack_archive(\n",
        "    os.path.join(download_base_dir, \"caltech-101\", \"101_ObjectCategories.tar.gz\"), \".\"\n",
        ")\n",
        "shutil.unpack_archive(\n",
        "    os.path.join(download_base_dir, \"caltech-101\", \"Annotations.tar\"), \".\"\n",
        ")\n",
        "\n",
        "# list of paths to images and annotations\n",
        "image_paths = [\n",
        "    f for f in os.listdir(path_images) if os.path.isfile(os.path.join(path_images, f))\n",
        "]\n",
        "annot_paths = [\n",
        "    f for f in os.listdir(path_annot) if os.path.isfile(os.path.join(path_annot, f))\n",
        "]\n",
        "\n",
        "image_paths.sort()\n",
        "annot_paths.sort()\n",
        "\n",
        "image_size = 224  # resize input images to this size\n",
        "\n",
        "images, targets = [], []\n",
        "\n",
        "# loop over the annotations and images, preprocess them and store in lists\n",
        "for i in range(0, len(annot_paths)):\n",
        "    # Access bounding box coordinates\n",
        "    annot = scipy.io.loadmat(path_annot + annot_paths[i])[\"box_coord\"][0]\n",
        "\n",
        "    top_left_x, top_left_y = annot[2], annot[0]\n",
        "    bottom_right_x, bottom_right_y = annot[3], annot[1]\n",
        "\n",
        "    image = keras.utils.load_img(\n",
        "        path_images + image_paths[i],\n",
        "    )\n",
        "    (w, h) = image.size[:2]\n",
        "\n",
        "    # resize images\n",
        "    image = image.resize((image_size, image_size))\n",
        "\n",
        "    # convert image to array and append to list\n",
        "    images.append(keras.utils.img_to_array(image))\n",
        "\n",
        "    # apply relative scaling to bounding boxes as per given image and append to list\n",
        "    targets.append(\n",
        "        (\n",
        "            float(top_left_x) / w,\n",
        "            float(top_left_y) / h,\n",
        "            float(bottom_right_x) / w,\n",
        "            float(bottom_right_y) / h,\n",
        "        )\n",
        "    )\n",
        "\n",
        "# Convert the list to numpy array, split to train and test dataset\n",
        "(x_train), (y_train) = (\n",
        "    np.asarray(images[: int(len(images) * 0.8)]),\n",
        "    np.asarray(targets[: int(len(targets) * 0.8)]),\n",
        ")\n",
        "(x_test), (y_test) = (\n",
        "    np.asarray(images[int(len(images) * 0.8) :]),\n",
        "    np.asarray(targets[int(len(targets) * 0.8) :]),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSHyG2SzkhHF"
      },
      "source": [
        "## Implement multilayer-perceptron (MLP)\n",
        "\n",
        "We use the code from the Keras example\n",
        "[Image classification with Vision Transformer](https://keras.io/examples/vision/image_classification_with_vision_transformer/)\n",
        "as a reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mRI6UlIkhHF"
      },
      "outputs": [],
      "source": [
        "\n",
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=keras.activations.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQ5P3LtCkhHF"
      },
      "source": [
        "## Implement the patch creation layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8izUF6cMkhHF"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        input_shape = ops.shape(images)\n",
        "        batch_size = input_shape[0]\n",
        "        height = input_shape[1]\n",
        "        width = input_shape[2]\n",
        "        channels = input_shape[3]\n",
        "        num_patches_h = height // self.patch_size\n",
        "        num_patches_w = width // self.patch_size\n",
        "        patches = keras.ops.image.extract_patches(images, size=self.patch_size)\n",
        "        patches = ops.reshape(\n",
        "            patches,\n",
        "            (\n",
        "                batch_size,\n",
        "                num_patches_h * num_patches_w,\n",
        "                self.patch_size * self.patch_size * channels,\n",
        "            ),\n",
        "        )\n",
        "        return patches\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"patch_size\": self.patch_size})\n",
        "        return config\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dywh2ZdpkhHG"
      },
      "source": [
        "## Display patches for an input image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bf7DhSLCkhHG"
      },
      "outputs": [],
      "source": [
        "patch_size = 32  # Size of the patches to be extracted from the input images\n",
        "\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.imshow(x_train[0].astype(\"uint8\"))\n",
        "plt.axis(\"off\")\n",
        "\n",
        "patches = Patches(patch_size)(np.expand_dims(x_train[0], axis=0))\n",
        "print(f\"Image size: {image_size} X {image_size}\")\n",
        "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
        "print(f\"{patches.shape[1]} patches per image \\n{patches.shape[-1]} elements per patch\")\n",
        "\n",
        "\n",
        "n = int(np.sqrt(patches.shape[1]))\n",
        "plt.figure(figsize=(4, 4))\n",
        "for i, patch in enumerate(patches[0]):\n",
        "    ax = plt.subplot(n, n, i + 1)\n",
        "    patch_img = ops.reshape(patch, (patch_size, patch_size, 3))\n",
        "    plt.imshow(ops.convert_to_numpy(patch_img).astype(\"uint8\"))\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODFNGVuakhHG"
      },
      "source": [
        "## Implement the patch encoding layer\n",
        "\n",
        "The `PatchEncoder` layer linearly transforms a patch by projecting it into a\n",
        "vector of size `projection_dim`. It also adds a learnable position\n",
        "embedding to the projected vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NP3x-VoskhHG"
      },
      "outputs": [],
      "source": [
        "\n",
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super().__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    # Override function to avoid error while saving model\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update(\n",
        "            {\n",
        "                \"input_shape\": input_shape,\n",
        "                \"patch_size\": patch_size,\n",
        "                \"num_patches\": num_patches,\n",
        "                \"projection_dim\": projection_dim,\n",
        "                \"num_heads\": num_heads,\n",
        "                \"transformer_units\": transformer_units,\n",
        "                \"transformer_layers\": transformer_layers,\n",
        "                \"mlp_head_units\": mlp_head_units,\n",
        "            }\n",
        "        )\n",
        "        return config\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = ops.expand_dims(\n",
        "            ops.arange(start=0, stop=self.num_patches, step=1), axis=0\n",
        "        )\n",
        "        projected_patches = self.projection(patch)\n",
        "        encoded = projected_patches + self.position_embedding(positions)\n",
        "        return encoded\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIaE2srKkhHH"
      },
      "source": [
        "## Build the ViT model\n",
        "\n",
        "The ViT model has multiple Transformer blocks.\n",
        "The `MultiHeadAttention` layer is used for self-attention,\n",
        "applied to the sequence of image patches. The encoded patches (skip connection)\n",
        "and self-attention layer outputs are normalized and fed into a\n",
        "multilayer perceptron (MLP).\n",
        "The model outputs four dimensions representing\n",
        "the bounding box coordinates of an object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVMOZBpKkhHH"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_vit_object_detector(\n",
        "    input_shape,\n",
        "    patch_size,\n",
        "    num_patches,\n",
        "    projection_dim,\n",
        "    num_heads,\n",
        "    transformer_units,\n",
        "    transformer_layers,\n",
        "    mlp_head_units,\n",
        "):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    # Create patches\n",
        "    patches = Patches(patch_size)(inputs)\n",
        "    # Encode patches\n",
        "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "        # Skip connection 2.\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Create a [batch_size, projection_dim] tensor.\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    representation = layers.Flatten()(representation)\n",
        "    representation = layers.Dropout(0.3)(representation)\n",
        "    # Add MLP.\n",
        "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.3)\n",
        "\n",
        "    bounding_box = layers.Dense(4)(\n",
        "        features\n",
        "    )  # Final four neurons that output bounding box\n",
        "\n",
        "    # return Keras model.\n",
        "    return keras.Model(inputs=inputs, outputs=bounding_box)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9_UHYKqkhHH"
      },
      "source": [
        "## Run the experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcwFHUL9khHH"
      },
      "outputs": [],
      "source": [
        "\n",
        "def run_experiment(model, learning_rate, weight_decay, batch_size, num_epochs):\n",
        "    optimizer = keras.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    # Compile model.\n",
        "    model.compile(optimizer=optimizer, loss=keras.losses.MeanSquaredError())\n",
        "\n",
        "    checkpoint_filepath = \"vit_object_detector.weights.h5\"\n",
        "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"val_loss\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[\n",
        "            checkpoint_callback,\n",
        "            keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "input_shape = (image_size, image_size, 3)  # input image shape\n",
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 32\n",
        "num_epochs = 100\n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "projection_dim = 64\n",
        "num_heads = 4\n",
        "# Size of the transformer layers\n",
        "transformer_units = [\n",
        "    projection_dim * 2,\n",
        "    projection_dim,\n",
        "]\n",
        "transformer_layers = 4\n",
        "mlp_head_units = [2048, 1024, 512, 64, 32]  # Size of the dense layers\n",
        "\n",
        "\n",
        "history = []\n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "\n",
        "vit_object_detector = create_vit_object_detector(\n",
        "    input_shape,\n",
        "    patch_size,\n",
        "    num_patches,\n",
        "    projection_dim,\n",
        "    num_heads,\n",
        "    transformer_units,\n",
        "    transformer_layers,\n",
        "    mlp_head_units,\n",
        ")\n",
        "\n",
        "# Train model\n",
        "history = run_experiment(\n",
        "    vit_object_detector, learning_rate, weight_decay, batch_size, num_epochs\n",
        ")\n",
        "\n",
        "\n",
        "def plot_history(item):\n",
        "    plt.plot(history.history[item], label=item)\n",
        "    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(item)\n",
        "    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_history(\"loss\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93a0_rsOkhHH"
      },
      "source": [
        "## Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBjaUZydkhHI"
      },
      "outputs": [],
      "source": [
        "import matplotlib.patches as patches\n",
        "\n",
        "# Saves the model in current path\n",
        "vit_object_detector.save(\"vit_object_detector.keras\")\n",
        "\n",
        "\n",
        "# To calculate IoU (intersection over union, given two bounding boxes)\n",
        "def bounding_box_intersection_over_union(box_predicted, box_truth):\n",
        "    # get (x, y) coordinates of intersection of bounding boxes\n",
        "    top_x_intersect = max(box_predicted[0], box_truth[0])\n",
        "    top_y_intersect = max(box_predicted[1], box_truth[1])\n",
        "    bottom_x_intersect = min(box_predicted[2], box_truth[2])\n",
        "    bottom_y_intersect = min(box_predicted[3], box_truth[3])\n",
        "\n",
        "    # calculate area of the intersection bb (bounding box)\n",
        "    intersection_area = max(0, bottom_x_intersect - top_x_intersect + 1) * max(\n",
        "        0, bottom_y_intersect - top_y_intersect + 1\n",
        "    )\n",
        "\n",
        "    # calculate area of the prediction bb and ground-truth bb\n",
        "    box_predicted_area = (box_predicted[2] - box_predicted[0] + 1) * (\n",
        "        box_predicted[3] - box_predicted[1] + 1\n",
        "    )\n",
        "    box_truth_area = (box_truth[2] - box_truth[0] + 1) * (\n",
        "        box_truth[3] - box_truth[1] + 1\n",
        "    )\n",
        "\n",
        "    # calculate intersection over union by taking intersection\n",
        "    # area and dividing it by the sum of predicted bb and ground truth\n",
        "    # bb areas subtracted by  the interesection area\n",
        "\n",
        "    # return ioU\n",
        "    return intersection_area / float(\n",
        "        box_predicted_area + box_truth_area - intersection_area\n",
        "    )\n",
        "\n",
        "\n",
        "i, mean_iou = 0, 0\n",
        "\n",
        "# Compare results for 10 images in the test set\n",
        "# Compare results for 20 images in the test set\n",
        "#for input_image in x_test[:10]:\n",
        "np.random.seed(4567)\n",
        "index = np.random.choice(x_test.shape[0],\n",
        "                                  size=20,\n",
        "                                  replace=False)\n",
        "for input_image in x_test[index]:\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 15))\n",
        "    im = input_image\n",
        "\n",
        "    # Display the image\n",
        "    ax1.imshow(im.astype(\"uint8\"))\n",
        "    ax2.imshow(im.astype(\"uint8\"))\n",
        "\n",
        "    input_image = cv2.resize(\n",
        "        input_image, (image_size, image_size), interpolation=cv2.INTER_AREA\n",
        "    )\n",
        "    input_image = np.expand_dims(input_image, axis=0)\n",
        "    preds = vit_object_detector.predict(input_image)[0]\n",
        "\n",
        "    (h, w) = (im).shape[0:2]\n",
        "\n",
        "    top_left_x, top_left_y = int(preds[0] * w), int(preds[1] * h)\n",
        "\n",
        "    bottom_right_x, bottom_right_y = int(preds[2] * w), int(preds[3] * h)\n",
        "\n",
        "    box_predicted = [top_left_x, top_left_y, bottom_right_x, bottom_right_y]\n",
        "    # Create the bounding box\n",
        "    rect = patches.Rectangle(\n",
        "        (top_left_x, top_left_y),\n",
        "        bottom_right_x - top_left_x,\n",
        "        bottom_right_y - top_left_y,\n",
        "        facecolor=\"none\",\n",
        "        edgecolor=\"red\",\n",
        "        linewidth=1,\n",
        "    )\n",
        "    # Add the bounding box to the image\n",
        "    ax1.add_patch(rect)\n",
        "    ax1.set_xlabel(\n",
        "        \"Predicted: \"\n",
        "        + str(top_left_x)\n",
        "        + \", \"\n",
        "        + str(top_left_y)\n",
        "        + \", \"\n",
        "        + str(bottom_right_x)\n",
        "        + \", \"\n",
        "        + str(bottom_right_y)\n",
        "    )\n",
        "\n",
        "    top_left_x, top_left_y = int(y_test[i][0] * w), int(y_test[i][1] * h)\n",
        "\n",
        "    bottom_right_x, bottom_right_y = int(y_test[i][2] * w), int(y_test[i][3] * h)\n",
        "\n",
        "    box_truth = top_left_x, top_left_y, bottom_right_x, bottom_right_y\n",
        "\n",
        "    mean_iou += bounding_box_intersection_over_union(box_predicted, box_truth)\n",
        "    # Create the bounding box\n",
        "    rect = patches.Rectangle(\n",
        "        (top_left_x, top_left_y),\n",
        "        bottom_right_x - top_left_x,\n",
        "        bottom_right_y - top_left_y,\n",
        "        facecolor=\"none\",\n",
        "        edgecolor=\"red\",\n",
        "        linewidth=1,\n",
        "    )\n",
        "    # Add the bounding box to the image\n",
        "    ax2.add_patch(rect)\n",
        "    ax2.set_xlabel(\n",
        "        \"Target: \"\n",
        "        + str(top_left_x)\n",
        "        + \", \"\n",
        "        + str(top_left_y)\n",
        "        + \", \"\n",
        "        + str(bottom_right_x)\n",
        "        + \", \"\n",
        "        + str(bottom_right_y)\n",
        "        + \"\\n\"\n",
        "        + \"IoU\"\n",
        "        + str(bounding_box_intersection_over_union(box_predicted, box_truth))\n",
        "    )\n",
        "    i = i + 1\n",
        "\n",
        "#print(\"mean_iou: \" + str(mean_iou / len(x_test[:10])))\n",
        "print(\"mean_iou: \" + str(mean_iou / len(x_test[index])))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFlaVUAokhHI"
      },
      "source": [
        "This example demonstrates that a pure Transformer can be trained\n",
        "to predict the bounding boxes of an object in a given image,\n",
        "thus extending the use of Transformers to object detection tasks.\n",
        "The model can be improved further by tuning hyper-parameters and pre-training."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}